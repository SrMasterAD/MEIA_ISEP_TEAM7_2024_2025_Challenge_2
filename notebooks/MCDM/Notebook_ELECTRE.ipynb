{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd, numpy as np, datetime, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_liters(capacity):\n",
    "    capacity = str(capacity).strip().upper()\n",
    "    return float(re.sub(r'[^\\d.]', '', capacity)) / (100 if 'CL' in capacity else 1000 if 'ML' in capacity else 1) if any(unit in capacity for unit in ['CL', 'ML', 'LITRE', 'LTR', 'L']) else ''\n",
    "\n",
    "def preprocess_data(df):\n",
    "    if df.empty: return df\n",
    "    \n",
    "    # Process numeric columns\n",
    "    for col in ['Price', 'ABV', 'Capacity']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col].apply(lambda x: re.sub(r'[^\\d.]', '', str(x)).strip() or np.nan), errors='coerce')\n",
    "            df[col] = MinMaxScaler().fit_transform(df[[col]]).round(3) if df[col].notnull().any() else df[col]\n",
    "    \n",
    "    # Process Style and Characteristics columns\n",
    "    for field in ['Style', 'Characteristics']:\n",
    "        if field in df.columns:\n",
    "            df[field] = df[field].str.replace(r'[^\\w\\s&,]', '', regex=True).str.split('&' if field == 'Style' else ',').apply(lambda x: [item.strip() for item in x] if isinstance(x, list) else x)\n",
    "            max_len = df[field].apply(lambda x: len(x) if isinstance(x, list) else 0).max()\n",
    "            for i in range(1, max_len + 1):\n",
    "                df[f'{field.rstrip(\"s\")} {i}'] = df[field].apply(lambda x: x[i-1] if isinstance(x, list) and len(x) >= i else '')\n",
    "            df = df.drop(columns=[field])\n",
    "    \n",
    "    # Process Vintage\n",
    "    if 'Vintage' in df.columns:\n",
    "        current_year = datetime.datetime.now().year\n",
    "        df['Vintage'] = df['Vintage'].apply(lambda x: current_year if str(x).strip().upper() == 'NV' else (int(re.search(r'\\d{4}', str(x)).group(0)) if re.search(r'\\d{4}', str(x)) else np.nan))\n",
    "        valid_years = df['Vintage'][df['Vintage'] > 1900]\n",
    "        if not valid_years.empty:\n",
    "            df['Vintage'] = df['Vintage'].apply(lambda x: max(0, (x - current_year) / (valid_years.min() - current_year)) if pd.notna(x) else np.nan).round(2)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Main execution\n",
    "df = pd.read_csv('../datasets/WineDataset.csv')\n",
    "df_cleaned = preprocess_data(df)\n",
    "df_cleaned.to_csv('../datasets/cleaned_wines.csv', index=False)\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file1 = \"../datasets/updated_wines.csv\"\n",
    "file2 = \"../datasets/merged_wine_dataset.csv\"\n",
    "\n",
    "df1 = pd.read_csv(file1) \n",
    "df2 = pd.read_csv(file2) \n",
    "\n",
    "# Merge the datasets based on WineName and WineryName\n",
    "merged_df = df2.merge(df1[['WineName', 'WineryName', 'Ratings']], on=['WineName', 'WineryName'], how='left')\n",
    "\n",
    "# Save the new dataset\n",
    "output_file = \"../datasets/PLNTD_dataset.csv\"\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"PLNTD_dataset created and saved to {output_file}\")\n",
    "\n",
    "missing_ratings = merged_df[merged_df['Ratings'].isna()]\n",
    "\n",
    "#Testing purposes\n",
    "if not missing_ratings.empty:\n",
    "    print(\"WARNING: Some rows in the dataset are missing a rating.\")\n",
    "    print(missing_ratings)\n",
    "else:\n",
    "    print(\"SUCCESS: All rows have a rating.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prepatation and visualization\n",
    "\n",
    "In this cell is performed a initial data preparation, creating a copy of the original DataFrame and selecting specific columns.\n",
    "\n",
    "Two types of criteria are defined:\n",
    "\n",
    "- Benefit criteria: characteristics where higher values are better\n",
    "- Cost criteria: characteristics where lower values are better (such as price)\n",
    "\n",
    "Encoding is performed to convert categorical variables into numerical ones:\n",
    "\n",
    "- Acidity: scale from 1 to 3\n",
    "- Body of wine: scale from 1 to 5\n",
    "\n",
    "Numerical columns are cleaned by removing symbols and text to convert them to float type:\n",
    "\n",
    "- ABV: removes \"ABV\" and the \"%\" symbol\n",
    "- Price: removes \"£\" and \"per bottle\"\n",
    "\n",
    "A density plot (KDE plot) is created for each column before normalization to visualize the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df = merged_df.copy()\n",
    "columns_for_copy = ['ABV', 'Ratings', 'Body', 'Acidity', 'Price', 'WineName', 'WineryName']\n",
    "copy_df = copy_df[columns_for_copy]\n",
    "\n",
    "\n",
    "benefit_criteria = ['ABV', 'Ratings', 'Body', 'Acidity']\n",
    "cost_criteria = ['Price']\n",
    "print(\"Benefit Criteria:\", benefit_criteria)\n",
    "print(\"Cost Criteria:\", cost_criteria)\n",
    "\n",
    "# Convert categorical 'Acidity' to numeric values (encoding)\n",
    "# Convert categorical 'Acidity' and 'Body' to numeric values (encoding)\n",
    "acidity_mapping = {'Low': 1, 'Medium': 2, 'High': 3}\n",
    "body_mapping = {'Very light-bodied': 1, 'Light-bodied': 2, 'Medium-bodied': 3, 'Full-bodied': 4, 'Very full-bodied': 5}\n",
    "\n",
    "merged_df['Acidity'] = [acidity_mapping[val] for val in merged_df['Acidity']]\n",
    "merged_df['Body'] = [body_mapping[val] for val in merged_df['Body']]\n",
    "\n",
    "merged_df['ABV'] = merged_df['ABV'].str.replace('ABV ', '').str.replace('%', '').astype(float)\n",
    "merged_df['Price'] = merged_df['Price'].str.replace('£', '').str.replace('per bottle', '').astype(float)\n",
    "\n",
    "columns_to_keep = benefit_criteria + cost_criteria\n",
    "merged_df = merged_df[columns_to_keep]\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.kdeplot(merged_df[column])\n",
    "    plt.title(f'Before normalization - {column}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization methods\n",
    "\n",
    "Each normalization method has its advantages, and by using all four we can compare the results against each other and see if any method works better for the specific data\n",
    "\n",
    "- Min-Max Normalization: Rescales the data so that its values fall within the range [0, 1] using the minimum and maximum values of the dataset\n",
    "- Linear Max Normalization: Divides each variable value by the maximum value of that variable, scaling the data to the range [0, 1].\n",
    "- Logarithmic Normalization: Applies a logarithmic transformation to the data.\n",
    "- Vector Normalization: Transforms the original values into relative proportions, considering the total magnitude of the data, and allows comparing different criteria on a common scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def normalize_dataset_with_min_max(df, benefit_criteria, cost_criteria):\n",
    "    \"\"\"\n",
    "    Formula for Min-Max technique:\n",
    "    \n",
    "    For benefit criteria:\n",
    "    nij = (rij - rmin) / (rmax - rmin)\n",
    "    \n",
    "    For cost criteria:\n",
    "    nij = (rmax - rij) / (rmax - rmin)\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with data\n",
    "        benefit_criteria: List of benefit criteria\n",
    "        cost_criteria: List of cost criteria\n",
    "    \n",
    "    Returns:\n",
    "        Normalized DataFrame\n",
    "    \"\"\"\n",
    "    # Create a copy of DataFrame to avoid modifying the original\n",
    "    normalized_df = df.copy()\n",
    "    \n",
    "    # Normalize benefit criteria (Min-Max)\n",
    "    for criteria in benefit_criteria:\n",
    "        rmin = df[criteria].min()\n",
    "        rmax = df[criteria].max()\n",
    "        normalized_df[criteria] = (df[criteria] - rmin) / (rmax - rmin)\n",
    "\n",
    "    # Normalize cost criteria (Inverted Min-Max)\n",
    "    for criteria in cost_criteria:\n",
    "        rmin = df[criteria].min()\n",
    "        rmax = df[criteria].max()\n",
    "        normalized_df[criteria] = (rmax - df[criteria]) / (rmax - rmin)\n",
    "        \n",
    "    return normalized_df\n",
    "\n",
    "def normalize_dataset_with_linear_max(df, benefit_criteria, cost_criteria):\n",
    "    \"\"\"\n",
    "    Formula for Linear Max technique:\n",
    "    \n",
    "    For benefit criteria:\n",
    "    nij = rij / rmax\n",
    "    \n",
    "    For cost criteria:\n",
    "    nij = 1 - (rij / rmax)\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with data\n",
    "        benefit_criteria: List of benefit criteria\n",
    "        cost_criteria: List of cost criteria\n",
    "    \n",
    "    Returns:\n",
    "        Normalized DataFrame\n",
    "    \"\"\"\n",
    "    # Create a copy of DataFrame to avoid modifying the original\n",
    "    normalized_df = df.copy()\n",
    "    \n",
    "    # Normalize benefit criteria\n",
    "    for criteria in benefit_criteria:\n",
    "        rmax = df[criteria].max()\n",
    "        normalized_df[criteria] = df[criteria] / rmax\n",
    "        \n",
    "    # Normalize cost criteria\n",
    "    for criteria in cost_criteria:\n",
    "        rmax = df[criteria].max()\n",
    "        normalized_df[criteria] = 1 - (df[criteria] / rmax)\n",
    "        \n",
    "    return normalized_df\n",
    "\n",
    "def normalize_dataset_with_vector(df, benefit_criteria, cost_criteria):\n",
    "    \"\"\"\n",
    "    Vector normalization:\n",
    "    \n",
    "    For benefit criteria:\n",
    "    g'_j(a_i) = g_j(a_i) / sqrt(sum([g_j(a_k)]^2))\n",
    "    \n",
    "    For cost criteria:\n",
    "    g'_j(a_i) = (min(g_j) - g_j(a_i)) / sqrt(sum([min(g_j) - g_j(a_k)]^2))\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with data\n",
    "        benefit_criteria: List of benefit criteria\n",
    "        cost_criteria: List of cost criteria\n",
    "    \n",
    "    Returns:\n",
    "        Normalized DataFrame\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Create a copy of DataFrame to avoid modifying the original\n",
    "    normalized_df = df.copy()\n",
    "    \n",
    "    # Normalize benefit criteria\n",
    "    for criteria in benefit_criteria:\n",
    "        # Calculate denominator (square root of sum of squares)\n",
    "        denominator = np.sqrt(np.sum(df[criteria] ** 2))\n",
    "        # Apply normalization formula\n",
    "        normalized_df[criteria] = df[criteria] / denominator\n",
    "        \n",
    "    # Normalize cost criteria\n",
    "    for criteria in cost_criteria:\n",
    "        # Find minimum value of criteria\n",
    "    \n",
    "        # Calculate numerator (min - current value)\n",
    "        numerator = df[criteria]\n",
    "        # Calculate denominator (square root of sum of squared differences)\n",
    "        denominator = np.sqrt(np.sum(df[criteria] ** 2))\n",
    "        # Apply normalization formula\n",
    "        result = numerator / denominator\n",
    "\n",
    "        normalized_df[criteria] = 1 - result\n",
    "        \n",
    "    return normalized_df\n",
    "\n",
    "def normalize_dataset_with_logarithmic(df, benefit_criteria, cost_criteria):\n",
    "   \"\"\"\n",
    "   Logarithmic normalization:\n",
    "   \n",
    "   For benefit criteria:\n",
    "   g'_j(a_i) = log(g_j(a_i)) / log(max(g_j))\n",
    "   \n",
    "   For cost criteria:\n",
    "   g'_j(a_i) = log(min(g_j)) / log(g_j(a_i))\n",
    "   \n",
    "   Args:\n",
    "       df: DataFrame with data\n",
    "       benefit_criteria: List of benefit criteria\n",
    "       cost_criteria: List of cost criteria\n",
    "   \n",
    "   Returns:\n",
    "       Normalized DataFrame\n",
    "   \"\"\"\n",
    "   import numpy as np\n",
    "   \n",
    "   # Create a copy of DataFrame to avoid modifying the original\n",
    "   normalized_df = df.copy()\n",
    "   \n",
    "   # Normalize benefit criteria\n",
    "   for criteria in benefit_criteria:\n",
    "       # Find maximum value of criteria\n",
    "       max_value = df[criteria].max()\n",
    "       # Apply logarithmic normalization formula\n",
    "       # Add small value (eps) to avoid log(0)\n",
    "       eps = np.finfo(float).eps\n",
    "       normalized_df[criteria] = np.log(df[criteria] + eps) / np.log(max_value + eps)\n",
    "       \n",
    "   # Normalize cost criteria\n",
    "   for criteria in cost_criteria:\n",
    "       # Find minimum value of criteria\n",
    "       min_value = df[criteria].min()\n",
    "       # Apply logarithmic normalization formula\n",
    "       eps = np.finfo(float).eps\n",
    "       normalized_df[criteria] = np.log(min_value + eps) / np.log(df[criteria] + eps)\n",
    "       \n",
    "   return normalized_df\n",
    "\n",
    "# Apply normalization techniques\n",
    "normalized_data_min_max = normalize_dataset_with_min_max(merged_df, benefit_criteria, cost_criteria)\n",
    "normalized_data_linear_max = normalize_dataset_with_linear_max(merged_df, benefit_criteria, cost_criteria)\n",
    "normalized_data_vector = normalize_dataset_with_vector(merged_df, benefit_criteria, cost_criteria)\n",
    "normalized_data_logarithmic = normalize_dataset_with_logarithmic(merged_df, benefit_criteria, cost_criteria)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of normalized data\n",
    "\n",
    "For each normalization method:\n",
    "\n",
    "- Show the first rows of the normalized dataset\n",
    "- Calculate and present the minimum and maximum values for each column\n",
    "- Provide a complete view of the normalized dataset\n",
    "\n",
    "By visualizing the datasets, it is possible to compare the different normalization techniques and identify possible anomalies or patterns\n",
    "\n",
    "It is interesting to note that in the case of columns that were designated as cost criteria, their data distribution appears inverted when compared to the distribution visualized previously before normalization. The remaining distributions maintain the same shape as before being normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with all normalized datasets and their names\n",
    "normalized_datasets = [\n",
    "   (normalized_data_min_max, \"Min-Max\"),\n",
    "   (normalized_data_linear_max, \"Linear Max\"), \n",
    "   (normalized_data_vector, \"Vector\"),\n",
    "   (normalized_data_logarithmic, \"Logarithmic\")\n",
    "]\n",
    "\n",
    "# Show results for each normalization method\n",
    "for dataset, method_name in normalized_datasets:\n",
    "   print(f\"\\nFirst rows of normalized data ({method_name}):\")\n",
    "   print(dataset.head())\n",
    "   \n",
    "   print(f\"\\nMinimum and maximum values after normalization ({method_name}):\")\n",
    "   for column in dataset.columns:\n",
    "       print(f\"{column}:\")\n",
    "       print(f\"  Min: {dataset[column].min():.3f}\")\n",
    "       print(f\"  Max: {dataset[column].max():.3f}\")\n",
    "   \n",
    "   # Create visualizations for each column of the dataset\n",
    "   for column in dataset.columns:\n",
    "       plt.figure(figsize=(12, 4))\n",
    "       plt.subplot(1, 2, 1)\n",
    "       sns.kdeplot(dataset[column])\n",
    "       plt.title(f'{method_name} normalization - {column}')\n",
    "       plt.tight_layout()\n",
    "       plt.show()\n",
    "   \n",
    "   print(f\"\\nComplete dataset ({method_name}):\")\n",
    "   print(dataset)\n",
    "   print(\"\\n\" + \"=\"*50 + \"\\n\")  # Separator between different methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics for Normalizations\n",
    "\n",
    "Three different metrics are implemented to evaluate the quality and characteristics of the normalizations performed.\n",
    "\n",
    "- Standard Deviation: Measures the dispersion of normalized data and allows evaluation of variability in each column after normalization\n",
    "- Minkowski Distance: Calculates the average distance between all pairs of observations and evaluates how normalization affects spatial relationships between observations\n",
    "- Mean Squared Error (MSE): Quantifies the difference between original and normalized data. Provides a measure of the magnitude of change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_standard_deviation(normalized_df):\n",
    "    return normalized_df.std()\n",
    "\n",
    "def calculate_minkowski_distance(normalized_df, p=2):\n",
    "    from scipy.spatial.distance import minkowski\n",
    "    distances = []\n",
    "    for i in range(len(normalized_df) - 1):\n",
    "        for j in range(i + 1, len(normalized_df)):\n",
    "            distances.append(minkowski(normalized_df.iloc[i], normalized_df.iloc[j], p=p))\n",
    "    return np.mean(distances)  # Mean distance\n",
    "\n",
    "def calculate_mean_squared_error(normalized_df, original_df):\n",
    "    mse = ((normalized_df - original_df) ** 2).mean().mean()  # Mean Squared Error\n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Calculate metrics for each normalization method\n",
    "for dataset, method_name in normalized_datasets:\n",
    "    results[method_name] = {\n",
    "        'standard_deviation': calculate_standard_deviation(dataset),\n",
    "        'minkowski_distance': calculate_minkowski_distance(dataset),\n",
    "        'mean_squared_error': calculate_mean_squared_error(dataset, merged_df)\n",
    "    }\n",
    "\n",
    "# Show results\n",
    "for method_name, metrics in results.items():\n",
    "    print(f\"\\nResults for {method_name}:\")\n",
    "    print(\"Standard Deviation:\")\n",
    "    print(metrics['standard_deviation'])\n",
    "    print(\"\\nAverage Minkowski Distance:\")\n",
    "    print(f\"{metrics['minkowski_distance']:.4f}\")\n",
    "    print(\"\\nMean Squared Error:\")\n",
    "    print(f\"{metrics['mean_squared_error']:.4f}\")\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Create DataFrame with results for comparison\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare data for DataFrame\n",
    "comparison_data = []\n",
    "for method_name, metrics in results.items():\n",
    "    # Calculate mean of standard deviation\n",
    "    mean_std = metrics['standard_deviation'].mean()\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Method': method_name,\n",
    "        'Mean Standard Deviation': mean_std,\n",
    "        'Minkowski Distance': metrics['minkowski_distance'],\n",
    "        'Mean Squared Error': metrics['mean_squared_error']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Show comparative table\n",
    "print(\"\\nComparative Table:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Results visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Subplot for Mean Standard Deviation\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(comparison_df['Method'], comparison_df['Mean Standard Deviation'])\n",
    "plt.title('Mean Standard Deviation')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Subplot for Minkowski Distance\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(comparison_df['Method'], comparison_df['Minkowski Distance'])\n",
    "plt.title('Minkowski Distance')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Subplot for Mean Squared Error\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(comparison_df['Method'], comparison_df['Mean Squared Error'])\n",
    "plt.title('Mean Squared Error')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Attribution and Best Normalization Selection\n",
    "\n",
    "As could be observed from the previous metrics, the vectorization method is the normalization method that guarantees better integrity and fidelity with the data. Here, this normalization is selected and weights are applied to the respective criteria in order to proceed with the calculation of the matrices necessary to develop the ELECTRE algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vector normalized dataset from normalized_datasets list\n",
    "vector_normalized_data = [dataset for dataset, method_name in normalized_datasets if method_name == \"Vector\"][0]\n",
    "\n",
    "normalized_data = vector_normalized_data.copy()\n",
    "\n",
    "weights = {\n",
    "    'ABV': 0.10,\n",
    "    'Ratings': 0.35,\n",
    "    'Body': 0.10,\n",
    "    'Price': 0.25,\n",
    "    'Acidity': 0.10\n",
    "}\n",
    "# multiply each column by its weight\n",
    "\n",
    "for column in normalized_data.columns:\n",
    "    normalized_data[column] = normalized_data[column] * weights[column]\n",
    "\n",
    "# check scores\n",
    "standard_deviation = calculate_standard_deviation(normalized_data)\n",
    "minkowski_distance = calculate_minkowski_distance(normalized_data)\n",
    "mean_squared_error = calculate_mean_squared_error(normalized_data, merged_df)\n",
    "\n",
    "print(\"Standard Deviation:\")\n",
    "print(standard_deviation)\n",
    "print(\"\\nMinkowski Distance:\")\n",
    "print(minkowski_distance)\n",
    "print(\"\\nMean Squared Error:\")\n",
    "print(mean_squared_error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  def compare_alternatives(normalized_df, benefit_criteria, cost_criteria, weights):\n",
    "#      \"\"\"\n",
    "#      Compares each pair of alternatives and creates sets of superior and inferior values.\n",
    "#      \n",
    "#      Args:\n",
    "#          normalized_df: Normalized DataFrame with criteria\n",
    "#      \n",
    "#      Returns:\n",
    "#          superior_values_set: Dictionary with superior value sets\n",
    "#          inferior_values_set: Dictionary with inferior value sets\n",
    "#      \"\"\"\n",
    "#  \n",
    "#      n_alternatives = len(normalized_df)\n",
    "#      criteria = list(normalized_df.columns)\n",
    "#      \n",
    "#      # Arrays 3D inicializados con False\n",
    "#      superior = np.zeros((n_alternatives, n_alternatives), dtype=float)\n",
    "#      inferior = np.zeros((n_alternatives, n_alternatives), dtype=bool)\n",
    "#      \n",
    "#      for i in range(n_alternatives):\n",
    "#          for j in range(n_alternatives):\n",
    "#              if i != j:\n",
    "#                  for k, criterion in enumerate(criteria):\n",
    "#                      if normalized_df.iloc[i][criterion] > normalized_df.iloc[j][criterion] and criterion in benefit_criteria:\n",
    "#                          superior[i, j] = superior[i, j] + weights[criterion]\n",
    "#                      if normalized_df.iloc[i][criterion] < normalized_df.iloc[j][criterion] and criterion in cost_criteria:\n",
    "#                          superior[i, j] = superior[i, j] + weights[criterion]\n",
    "#                      else:\n",
    "#                          inferior[i, j] = True\n",
    "#                  \n",
    "#      return superior, inferior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concordance Matrix Calculation\n",
    "\n",
    "The concordance matrix is a fundamental part of the ELECTRE algorithm and represents, for each pair of alternatives (a,b), the degree to which alternative a is at least as good as alternative b. The calculation is performed by summing the weights of the criteria where alternative a is equal to or better than b. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_concordance_matrix(normalized_df, criteria_weights):\n",
    "    \"\"\"\n",
    "    Calculates the concordance matrix C.\n",
    "    \n",
    "    Args:\n",
    "        normalized_df: DataFrame with normalized values\n",
    "        concordance_sets: Dictionary with concordance sets\n",
    "        criteria_weights: Dictionary with criteria weights (optional)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with the concordance matrix\n",
    "    \"\"\"\n",
    "    n_alternatives = len(normalized_df)\n",
    "    criteria = list(normalized_df.columns)\n",
    "    \n",
    "    # Initialize concordance matrix\n",
    "    concordance_matrix = np.zeros((n_alternatives, n_alternatives))\n",
    "    \n",
    "    # Calculate each element Cij\n",
    "    for i in range(n_alternatives):\n",
    "        for j in range(n_alternatives):\n",
    "            if i != j:  \n",
    "                # Sum the weights of the criteria in the concordance set\n",
    "                concordance_matrix[i, j] = sum(criteria_weights[criterion] \n",
    "                                               for criterion in criteria \n",
    "                                               if (normalized_df.iloc[i][criterion] > normalized_df.iloc[j][criterion]))\n",
    "                    \n",
    "\n",
    "    # Calculate the sum of all elements\n",
    "    sum_elements = np.sum(concordance_matrix)\n",
    "\n",
    "    # Calculate the total number of elements\n",
    "    num_elements = concordance_matrix.size - n_alternatives\n",
    "\n",
    "    # Calculate C_BAR\n",
    "    c_bar = sum_elements / num_elements\n",
    "\n",
    "    # Generate the new matrix by comparing with C_BAR\n",
    "    binary_matrix = (concordance_matrix >= c_bar).astype(int)\n",
    "\n",
    "    return pd.DataFrame(binary_matrix, \n",
    "                        index=[f'A{i+1}' for i in range(n_alternatives)],\n",
    "                        columns=[f'A{i+1}' for i in range(n_alternatives)])\n",
    "\n",
    "# Calculate the matrices\n",
    "concordance_matrix = calculate_concordance_matrix(normalized_data, weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discordance Matrix Calculation\n",
    "\n",
    "The discordance matrix in ELECTRE measures the degree of disagreement or 'veto' between alternatives. Discordance measures how strong the evidence is against the claim 'a is at least as good as b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_discordance_matrix(normalized_df):\n",
    "    \"\"\"\n",
    "    Calculates the distance matrix between all alternatives.\n",
    "    \n",
    "    Parameters:\n",
    "    normalized_df (pandas.DataFrame): Normalized DataFrame with alternatives as indices and criteria as columns\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Distance matrix between all alternatives\n",
    "    \"\"\"\n",
    "    n_alternatives = len(normalized_df)\n",
    "    \n",
    "    # Create an empty DataFrame to store distances\n",
    "    distances_df = np.zeros((n_alternatives, n_alternatives))\n",
    "    \n",
    "    for i in range(n_alternatives):\n",
    "        for j in range(n_alternatives):\n",
    "            if i != j:\n",
    "    \n",
    "                # Calculate all differences in all criteria between alternatives i and j, and store them in a list\n",
    "                differences = [normalized_df.iloc[i][c] - normalized_df.iloc[j][c] for c in normalized_df.columns]\n",
    "\n",
    "                # Calculate the max negative difference\n",
    "                max_neg_diff = min(differences)\n",
    "\n",
    "                # Calculate the greatest absolute difference\n",
    "                max_diff = max([abs(d) for d in differences])\n",
    "\n",
    "                # Calculate the distance between alternatives i and j\n",
    "                distances_df[i, j] = abs(max_neg_diff) / max_diff if max_diff != 0 else 0\n",
    "            \n",
    "    # Calculate the sum of all elements\n",
    "    sum_elements = np.sum(distances_df)\n",
    "\n",
    "    # Calculate the total number of elements\n",
    "    num_elements = distances_df.size - n_alternatives\n",
    "\n",
    "    # Calculate D_BAR\n",
    "    d_bar = sum_elements / num_elements\n",
    "    \n",
    "    # Generate the binary matrix\n",
    "    binary_matrix = (distances_df >= d_bar).astype(int)\n",
    "    \n",
    "    return pd.DataFrame(binary_matrix, \n",
    "                        index=[f'A{i+1}' for i in range(n_alternatives)],\n",
    "                        columns=[f'A{i+1}' for i in range(n_alternatives)])\n",
    "    \n",
    "# Calculate distances\n",
    "discordance_matrix = calculate_discordance_matrix(normalized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation of Concordance and Discordance Matrices\n",
    "\n",
    "The discordance matrix is used together with the concordance matrix to determine the outranking relationships between alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_concordance_discordance_aggregation_matrix(concordance_matrix, discordance_matrix):\n",
    "    \"\"\"\n",
    "    Calculates the global concordance matrix C from the concordance and discordance matrices.\n",
    "    \n",
    "    Parameters:\n",
    "    concordance_matrix (pandas.DataFrame): Binary concordance matrix\n",
    "    discordance_matrix (pandas.DataFrame): Binary discordance matrix\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Global concordance matrix\n",
    "    \"\"\"\n",
    "    # Calculate the global concordance matrix\n",
    "    global_concordance_matrix = concordance_matrix & discordance_matrix\n",
    "    \n",
    "    return global_concordance_matrix\n",
    "\n",
    "# Calculate the aggregation matrix\n",
    "aggregation_matrix = calculate_concordance_discordance_aggregation_matrix(concordance_matrix, discordance_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking of Alternatives\n",
    "\n",
    "In this cell, a ranking of alternatives is created based on an aggregation matrix. If it finds a 1 in this matrix at position (i,j), it means that alternative i dominates j\n",
    "\n",
    "- Increases the dominance counter for i\n",
    "- Increases the 'times dominated' counter for j\n",
    "\n",
    "Sorts the alternatives based on two criteria: First by the number of dominances (higher is better). In case of a tie, by the lowest number of times dominated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ranking(matrix):\n",
    "    n = len(matrix)\n",
    "    dominance = [0] * n\n",
    "    dominated = [0] * n\n",
    "\n",
    "    for index in range(len(matrix)):\n",
    "        row_array = matrix.iloc[index].to_numpy()  # Convertir la fila en array\n",
    "        row_list = row_array.tolist()  # Convertir el array de NumPy en lista Python\n",
    "        dominance[index] = row_list.count(1)  # Ahora contamos 1.0 en lugar de 1\n",
    "        dominated[index] = row_list.count(0) - 1  # Contamos 0.0 en lugar de 0\n",
    "\n",
    "\n",
    "    # Create ranking based on dominance and dominated\n",
    "    ranking = sorted(range(n), key=lambda x: (-dominance[x], dominated[x]))\n",
    "    \n",
    "    # Create a dictionary mapping alternative index to rank\n",
    "    # Modificación sugerida\n",
    "    rank_dict = {alt: pos for pos, alt in enumerate(ranking, start=1)}  # Usar el índice directamente sin sumar 1    \n",
    "\n",
    "    return rank_dict, dominance, dominated\n",
    "\n",
    "# Calculate ranking\n",
    "ranking, dominance, dominated = calculate_ranking(aggregation_matrix)\n",
    "\n",
    "# Display ranking\n",
    "print(\"Ranking:\")\n",
    "for pos, alt in enumerate(ranking, start=1):\n",
    "    print(f\"A{alt+1}: Position = {pos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear null values from the dataset\n",
    "copy_df = copy_df.dropna()\n",
    "\n",
    "# Calculate ranking\n",
    "ranking, dominance, dominated = calculate_ranking(aggregation_matrix)\n",
    "\n",
    "# Add ranking to merged_df\n",
    "copy_df['Ranking'] = merged_df.index.map(lambda x: ranking.get(x))\n",
    "\n",
    "# Sort the dataframe by ranking\n",
    "copy_df = copy_df.sort_values('Ranking')\n",
    "\n",
    "# Display the ranked dataframe\n",
    "print(copy_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'ABV': [17, 5, 13, 9, 21],\n",
    "    'Ratings': [3, 5, 2, 4, 1],\n",
    "    'Body': [5, 2, 4, 3, 1],\n",
    "    'Acidity': [1, 3, 1, 2, 3],\n",
    "    'Price': [50, 30, 20, 10, 5]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data, index=['A1', 'A2', 'A3', 'A4', 'A5'])\n",
    "\n",
    "benefit_criteria = ['ABV', 'Ratings', 'Body', 'Acidity']\n",
    "cost_criteria = ['Price']\n",
    "normalized_df = normalize_dataset_with_vector(df, benefit_criteria, cost_criteria)\n",
    "\n",
    "weights = {\n",
    "    'ABV': 0.10,\n",
    "    'Ratings': 0.35,\n",
    "    'Body': 0.10,\n",
    "    'Price': 0.25,\n",
    "    'Acidity': 0.10\n",
    "}\n",
    "\n",
    "for column in normalized_df.columns:\n",
    "    normalized_df[column] = normalized_df[column] * weights[column]\n",
    "\n",
    "concordance_matrix = calculate_concordance_matrix(normalized_df, weights)\n",
    "discordance_matrix = calculate_discordance_matrix(normalized_df)\n",
    "aggregation_matrix = calculate_concordance_discordance_aggregation_matrix(concordance_matrix, discordance_matrix)\n",
    "\n",
    "print(\"Aggregation Matrix:\")\n",
    "print(aggregation_matrix)\n",
    "rank_dict, dominance, dominated = calculate_ranking(aggregation_matrix)\n",
    "\n",
    "print(\"Dominance:\", dominance)\n",
    "print(\"Dominated:\", dominated)\n",
    "\n",
    "print(\"Ranking:\", rank_dict)\n",
    "normalized_df['Ranking'] = normalized_df.index.map(lambda x: rank_dict.get(x))\n",
    "normalized_df = normalized_df.sort_values('Ranking')\n",
    "print(normalized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  normalized_df = normalized_df.drop(columns=['Ranking'])\n",
    "#  superior, inferior = compare_alternatives(normalized_df, benefit_criteria, cost_criteria, weights)\n",
    "#  \n",
    "#  print(\"Superior Values Set:\")\n",
    "#  print(superior)\n",
    "#  print(\"\\nInferior Values Set:\")\n",
    "#  print(inferior)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
