{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Data Preprocessing - Task [002]\n",
    "\n",
    "## Purpose\n",
    "\n",
    "1. Splitting text components for better granularity.\n",
    "2. Normalizing numerical values to ensure consistency.\n",
    "3. Removing unnecessary symbols or irrelevant text.\n",
    "\n",
    "The preprocessing will address these specific attributes in the dataset:\n",
    "- **Style**\n",
    "- **Characteristics**\n",
    "- **Price**\n",
    "- **Capacity**\n",
    "- **ABV (Alcohol by Volume)**\n",
    "- **Vintage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '../datasets/WineDataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "def convert_to_liters(capacity):\n",
    "    capacity = str(capacity).strip().upper()\n",
    "    if 'CL' in capacity:  # Centiliters to Liters\n",
    "        return float(re.sub(r'[^\\d.]', '', capacity)) / 100\n",
    "    elif 'ML' in capacity:  # Milliliters to Liters\n",
    "        return float(re.sub(r'[^\\d.]', '', capacity)) / 1000\n",
    "    elif 'LITRE' in capacity or 'L' in capacity:  # Liters already\n",
    "        return float(re.sub(r'[^\\d.]', '', capacity))\n",
    "    elif 'LTR' in capacity or 'L' in capacity:  # Liters already\n",
    "        return float(re.sub(r'[^\\d.]', '', capacity))\n",
    "    elif 'L' in capacity or 'L' in capacity:  # Liters already\n",
    "        return float(re.sub(r'[^\\d.]', '', capacity))\n",
    "    else:\n",
    "        return ''  # Handle any unknown format\n",
    "\n",
    "def preprocess_data(df):\n",
    "\n",
    "    numeric_cols = ['Price', 'ABV', 'Capacity']\n",
    "\n",
    "    df['Capacity'] = df['Capacity'].apply(convert_to_liters)\n",
    "\n",
    "    if not df.empty:\n",
    "        for col in numeric_cols:\n",
    "            if col in df.columns:\n",
    "                # Remove non-numeric characters and convert to float\n",
    "                df[col] = df[col].apply(lambda x: re.sub(r'[^\\d.]', '', str(x)).strip() if str(x).strip() else np.nan)\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                \n",
    "                if df[col].notnull().any():  # Check if there's valid data for scaling\n",
    "                    scaler = MinMaxScaler()\n",
    "                    df[col] = scaler.fit_transform(df[[col]])\n",
    "                \n",
    "                df[col] = df[col].round(3)\n",
    "\n",
    "        # Clean and split the 'Style' column\n",
    "        if 'Style' in df.columns:\n",
    "            df['Style'] = (\n",
    "                df['Style']\n",
    "                .str.replace(r'[^\\w\\s&]', '', regex=True)\n",
    "                .str.split('&')\n",
    "                .apply(lambda x: [item.strip() for item in x] if isinstance(x, list) else x)  # Clean whitespace\n",
    "            )\n",
    "\n",
    "            # This code divides the 'Style' array into several columns, each representing a position in that array\n",
    "            max_len = df['Style'].apply(lambda x: len(x) if isinstance(x, list) else 0).max()\n",
    "\n",
    "            for i in range(1, max_len + 1):\n",
    "                df[f'Style {i}'] = df['Style'].apply(lambda x: x[i-1] if isinstance(x, list) and len(x) >= i else '')\n",
    "\n",
    "            df = df.drop(columns=['Style'])\n",
    "\n",
    "        # Clean and split the 'Characteristics' column\n",
    "        if 'Characteristics' in df.columns:\n",
    "            df['Characteristics'] = (\n",
    "                df['Characteristics']\n",
    "                .str.replace(r'[^\\w\\s,]', '', regex=True)\n",
    "                .str.split(',') \n",
    "                .apply(lambda x: [item.strip() for item in x] if isinstance(x, list) else x)  # Clean whitespace\n",
    "            )\n",
    "            \n",
    "            # This code divides the 'Characteristics' array into several columns, each representing a position in that array\n",
    "            max_len = df['Characteristics'].apply(lambda x: len(x) if isinstance(x, list) else 0).max()\n",
    "\n",
    "            for i in range(1, max_len + 1):\n",
    "                df[f'Characteristic {i}'] = df['Characteristics'].apply(lambda x: x[i-1] if isinstance(x, list) and len(x) >= i else '')\n",
    "\n",
    "            df = df.drop(columns=['Characteristics'])\n",
    "            \n",
    "        # Clean and normalize the 'Vintage' column\n",
    "        if 'Vintage' in df.columns:\n",
    "            current_year = datetime.datetime.now().year\n",
    "\n",
    "            df['Vintage'] = df['Vintage'].apply(\n",
    "                lambda x: current_year if str(x).strip().upper() == 'NV' else (int(re.search(r'\\d{4}', str(x)).group(0)) if re.search(r'\\d{4}', str(x)) else np.nan)\n",
    "            )\n",
    "\n",
    "            valid_years = df['Vintage'][df['Vintage'] > 1900]\n",
    "            if not valid_years.empty:\n",
    "\n",
    "                min_year = valid_years.min()  \n",
    "                max_year = current_year\n",
    "\n",
    "                # Calculates the vintage value based on the max vintage and the current year\n",
    "                df['Vintage'] = df['Vintage'].apply(\n",
    "                    lambda x: max(0, (x - max_year) / (min_year - max_year)) if pd.notna(x) else np.nan\n",
    "                )\n",
    "\n",
    "                # Round the 'Vintage' values to 2 decimal places\n",
    "                df['Vintage'] = df['Vintage'].round(2)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Preprocess the dataset\n",
    "df_cleaned = preprocess_data(df)\n",
    "\n",
    "# Save or display the cleaned dataset\n",
    "df_cleaned.to_csv('../datasets/cleaned_wines.csv', index=False)\n",
    "df_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Data Preprocessing - Task [92]\n",
    "\n",
    "## Purpose\n",
    "\n",
    "1. Merging both updated_wines.csv that has the mean ratings, with the merged_wine_dataset that was the result of Report 2(id:65). Adding the rating of the first dataset to the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file1 = \"../datasets/updated_wines.csv\"\n",
    "file2 = \"../datasets/merged_wine_dataset.csv\"\n",
    "\n",
    "df1 = pd.read_csv(file1) \n",
    "df2 = pd.read_csv(file2) \n",
    "\n",
    "# Merge the datasets based on WineName and WineryName\n",
    "merged_df = df2.merge(df1[['WineName', 'WineryName', 'Ratings']], on=['WineName', 'WineryName'], how='left')\n",
    "\n",
    "# Save the new dataset\n",
    "output_file = \"../datasets/PLNTD_dataset.csv\"\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"PLNTD_dataset created and saved to {output_file}\")\n",
    "\n",
    "missing_ratings = merged_df[merged_df['Ratings'].isna()]\n",
    "\n",
    "#Testing purposes\n",
    "if not missing_ratings.empty:\n",
    "    print(\"WARNING: Some rows in the dataset are missing a rating.\")\n",
    "    print(missing_ratings)\n",
    "else:\n",
    "    print(\"SUCCESS: All rows have a rating.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELECTRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define benefit_criteria\n",
    "\n",
    "\n",
    "benefit_criteria = ['ABV', 'Ratings', 'Body']\n",
    "cost_criteria = ['Price', 'Acidity']\n",
    "print(\"Benefit Criteria:\", benefit_criteria)\n",
    "print(\"Cost Criteria:\", cost_criteria)\n",
    "\n",
    "# Convert categorical 'Acidity' to numeric values (encoding)\n",
    "# Convert categorical 'Acidity' and 'Body' to numeric values (encoding)\n",
    "acidity_mapping = {'Low': 1, 'Medium': 2, 'High': 3}\n",
    "body_mapping = {'Very light-bodied': 1, 'Light-bodied': 2, 'Medium-bodied': 3, 'Full-bodied': 4, 'Very full-bodied': 5}\n",
    "\n",
    "merged_df['Acidity'] = [acidity_mapping[val] for val in merged_df['Acidity']]\n",
    "merged_df['Body'] = [body_mapping[val] for val in merged_df['Body']]\n",
    "\n",
    "merged_df['ABV'] = merged_df['ABV'].str.replace('ABV ', '').str.replace('%', '').astype(float)\n",
    "merged_df['Price'] = merged_df['Price'].str.replace('Â£', '').str.replace('per bottle', '').astype(float)\n",
    "\n",
    "columns_to_keep = benefit_criteria + cost_criteria\n",
    "merged_df = merged_df[columns_to_keep]\n",
    "\n",
    "\n",
    "for column in merged_df.columns:\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.kdeplot(merged_df[column])\n",
    "    plt.title(f'Before normalization - {column}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(merged_df)\n",
    "\n",
    "\n",
    "print(\"Normalizing DataFrame:\")\n",
    "scaler = MinMaxScaler()\n",
    "normalized_df = pd.DataFrame(scaler.fit_transform(merged_df), columns=merged_df.columns)\n",
    "print(normalized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Calculate concordance matrix\n",
    "print(\"Calculating Concordance Matrix...\")\n",
    "# Ensure weights are a numpy array\n",
    "# Define weights (customize these based on your preferences)\n",
    "weights = {\n",
    "    'ABV': 0.15,\n",
    "    'Ratings': 0.25,\n",
    "    'Body': 0.2,\n",
    "    'Price': 0.25,\n",
    "    'Acidity': 0.15\n",
    "}\n",
    "# Define thresholds for each criterion\n",
    "thresholds = {\n",
    "    'ABV': {\n",
    "        'q': 0.05,  # Small ABV differences matter\n",
    "        'p': 0.15,  # Clear preference for notably different ABVs\n",
    "        'v': 0.3    # Veto for very different ABVs\n",
    "    },\n",
    "    'Ratings': {\n",
    "        'q': 0.02,  # Even small rating differences matter\n",
    "        'p': 0.1,   # Clear preference for better rated wines\n",
    "        'v': 0.25   # Strong veto for very different ratings\n",
    "    },\n",
    "    'Body': {\n",
    "        'q': 0.1,   # Some body difference can be negligible\n",
    "        'p': 0.2,   # Clear preference for desired body\n",
    "        'v': 0.4    # Veto only for extreme differences\n",
    "    },\n",
    "    'Price': {\n",
    "        'q': 0.03,  # Small price differences matter\n",
    "        'p': 0.12,  # Clear preference for better value\n",
    "        'v': 0.3    # Veto for very expensive differences\n",
    "    },\n",
    "    'Acidity': {\n",
    "        'q': 0.15,  # Less sensitive to acidity differences\n",
    "        'p': 0.25,  # Need bigger difference for preference\n",
    "        'v': 0.5    # Veto only for extreme differences\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get the number of alternatives\n",
    "alternatives = normalized_df.index\n",
    "n = len(alternatives)\n",
    "# Initialize the concordance matrix\n",
    "concordance_matrix = pd.DataFrame(np.zeros((n, n)), index=alternatives, columns=alternatives)\n",
    "# Calculate the concordance index for each pair of alternatives\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i != j:\n",
    "            concordance_sum = 0\n",
    "            for criterion in normalized_df.columns:\n",
    "                diff = normalized_df.iloc[i][criterion] - normalized_df.iloc[j][criterion]\n",
    "                \n",
    "                # Get thresholds for this criterion\n",
    "                q = thresholds[criterion]['q']  # indifference threshold\n",
    "                p = thresholds[criterion]['p']  # preference threshold\n",
    "                \n",
    "                # Calculate concordance considering criterion type\n",
    "                if criterion in benefit_criteria:\n",
    "                    if diff >= -q:\n",
    "                        concordance_sum += weights[criterion]\n",
    "                    elif diff < -p:\n",
    "                        concordance_sum += 0\n",
    "                    else:\n",
    "                        concordance_sum += weights[criterion] * (diff + p) / (p - q)\n",
    "                else:  # cost criteria\n",
    "                    if diff <= q:\n",
    "                        concordance_sum += weights[criterion]\n",
    "                    elif diff > p:\n",
    "                        concordance_sum += 0\n",
    "                    else:\n",
    "                        concordance_sum += weights[criterion] * (p - diff) / (p - q)\n",
    "            \n",
    "            concordance_matrix.iloc[i, j] = concordance_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Calculate discordance matrix\n",
    "print(\"Calculating Discordance Matrix...\")\n",
    "# Calculate the range (d_k) for each criterion\n",
    "ranges = normalized_df.max() - normalized_df.min()\n",
    "\n",
    "# Initialize a dictionary to store discordance matrices for each criterion\n",
    "discordance_matrices = {}\n",
    "\n",
    "for criterion in normalized_df.columns:\n",
    "    discordance_matrix = np.zeros((n, n))\n",
    "    v = thresholds[criterion]['v']  # veto threshold\n",
    "    p = thresholds[criterion]['p']  # preference threshold\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                diff = normalized_df.iloc[i][criterion] - normalized_df.iloc[j][criterion]\n",
    "                \n",
    "                if criterion in benefit_criteria:\n",
    "                    if diff <= -p:\n",
    "                        discordance_matrix[i, j] = min(1, (-diff - p) / (v - p))\n",
    "                else:  # cost criteria\n",
    "                    if diff >= p:\n",
    "                        discordance_matrix[i, j] = min(1, (diff - p) / (v - p))\n",
    "                        \n",
    "    discordance_matrices[criterion] = discordance_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate credibility matrix\n",
    "print(\"Calculating Credibility Matrix...\")\n",
    "credibility_matrix = np.zeros((n, n))\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i != j:\n",
    "            # Get maximum discordance value\n",
    "            max_disc = max(discordance_matrices[criterion][i,j] for criterion in merged_df.columns)\n",
    "            \n",
    "            if max_disc > concordance_matrix.iloc[i,j]:\n",
    "                credibility_matrix[i,j] = 0\n",
    "            else:\n",
    "                product = 1\n",
    "                for criterion in merged_df.columns:\n",
    "                    if discordance_matrices[criterion][i,j] > concordance_matrix.iloc[i,j]:\n",
    "                        product *= (1 - discordance_matrices[criterion][i,j]) / (1 - concordance_matrix.iloc[i,j])\n",
    "                credibility_matrix[i,j] = concordance_matrix.iloc[i,j] * product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate dominance matrix\n",
    "print(\"Generating Dominance Matrix...\")\n",
    "c_threshold = 0.6  # Example threshold for concordance\n",
    "d_threshold = 0.4  # Example threshold for discordance\n",
    "print(f\"Concordance Threshold: {c_threshold}, Discordance Threshold: {d_threshold}\")\n",
    "\n",
    "# Ensure the matrices are numpy arrays\n",
    "concordance_matrix = np.array(concordance_matrix)\n",
    "discordance_matrix = np.array(discordance_matrix)\n",
    "# Get the number of alternatives\n",
    "n = concordance_matrix.shape[0]\n",
    "# Initialize the dominance matrix with zeros\n",
    "dominance_matrix = np.zeros((n, n), dtype=int)\n",
    "# Iterate over all pairs of alternatives\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i != j:  # Exclude self-comparison\n",
    "            if (concordance_matrix[i, j] >= c_threshold and\n",
    "                    discordance_matrix[i, j] <= d_threshold):\n",
    "                dominance_matrix[i, j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominance_scores = dominance_matrix.sum(axis=1)\n",
    "print(\"Dominance Scores:\")\n",
    "print(dominance_scores)\n",
    "\n",
    "ranked_alternatives = np.argsort(-dominance_scores)  # Descending order\n",
    "print(\"Ranked Alternatives (Best to Worst):\")\n",
    "print(ranked_alternatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distillation procedures\n",
    "def calculate_qualification_score(credibility_matrix, cutoff):\n",
    "    n = len(credibility_matrix)\n",
    "    qualification = np.zeros(n)\n",
    "    \n",
    "    for i in range(n):\n",
    "        strength = sum(1 for j in range(n) if credibility_matrix[i,j] > cutoff)\n",
    "        weakness = sum(1 for j in range(n) if credibility_matrix[j,i] > cutoff)\n",
    "        qualification[i] = strength - weakness\n",
    "    \n",
    "    return qualification\n",
    "\n",
    "# Descending distillation\n",
    "def descending_distillation(credibility_matrix):\n",
    "    n = len(credibility_matrix)\n",
    "    ranking = []\n",
    "    remaining = list(range(n))\n",
    "    \n",
    "    while remaining:\n",
    "        cutoff = 0.7  # Initial cutoff value\n",
    "        while True:\n",
    "            qualification = calculate_qualification_score(credibility_matrix[np.ix_(remaining, remaining)], cutoff)\n",
    "            if max(qualification) - min(qualification) > 0:\n",
    "                break\n",
    "            cutoff -= 0.05\n",
    "            if cutoff < 0:\n",
    "                break\n",
    "        \n",
    "        max_qual = max(qualification)\n",
    "        max_indices = [remaining[i] for i in range(len(qualification)) if qualification[i] == max_qual]\n",
    "        ranking.extend(max_indices)\n",
    "        remaining = [i for i in remaining if i not in max_indices]\n",
    "    \n",
    "    return ranking\n",
    "\n",
    "# Ascending distillation\n",
    "def ascending_distillation(credibility_matrix):\n",
    "    n = len(credibility_matrix)\n",
    "    ranking = []\n",
    "    remaining = list(range(n))\n",
    "    \n",
    "    while remaining:\n",
    "        cutoff = 0.7\n",
    "        while True:\n",
    "            qualification = calculate_qualification_score(credibility_matrix[np.ix_(remaining, remaining)], cutoff)\n",
    "            if max(qualification) - min(qualification) > 0:\n",
    "                break\n",
    "            cutoff -= 0.05\n",
    "            if cutoff < 0:\n",
    "                break\n",
    "        \n",
    "        min_qual = min(qualification)\n",
    "        min_indices = [remaining[i] for i in range(len(qualification)) if qualification[i] == min_qual]\n",
    "        ranking = min_indices + ranking\n",
    "        remaining = [i for i in remaining if i not in min_indices]\n",
    "    \n",
    "    return ranking\n",
    "\n",
    "# Perform both distillations\n",
    "descending_ranking = descending_distillation(credibility_matrix)\n",
    "ascending_ranking = ascending_distillation(credibility_matrix)\n",
    "\n",
    "# Final ranking (median position from both rankings)\n",
    "final_ranking = []\n",
    "for i in range(n):\n",
    "    desc_pos = descending_ranking.index(i)\n",
    "    asc_pos = ascending_ranking.index(i)\n",
    "    final_ranking.append((i, (desc_pos + asc_pos) / 2))\n",
    "\n",
    "final_ranking.sort(key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved visualization function\n",
    "def visualize_electre_results(concordance_matrix, credibility_matrix, final_ranking, normalized_df):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # Concordance matrix heatmap\n",
    "    plt.subplot(231)\n",
    "    sns.heatmap(concordance_matrix, cmap='YlOrRd', center=0.5)\n",
    "    plt.title('Concordance Matrix')\n",
    "    \n",
    "    # Credibility matrix heatmap\n",
    "    plt.subplot(232)\n",
    "    sns.heatmap(credibility_matrix, cmap='YlOrRd')\n",
    "    plt.title('Credibility Matrix')\n",
    "    \n",
    "    # Ranking visualization\n",
    "    plt.subplot(233)\n",
    "    ranking_df = pd.DataFrame([x[1] for x in final_ranking], \n",
    "                            index=[x[0] for x in final_ranking])\n",
    "    sns.barplot(x=ranking_df.index, y=ranking_df[0], color='skyblue')\n",
    "    plt.title('Final Ranking (lower is better)')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Distribution of criteria values for top 5 alternatives\n",
    "    plt.subplot(234)\n",
    "    top_5 = [x[0] for x in final_ranking[:5]]\n",
    "    for criterion in normalized_df.columns:\n",
    "        plt.plot(normalized_df.loc[top_5, criterion], label=criterion)\n",
    "    plt.legend()\n",
    "    plt.title('Criteria Values for Top 5 Alternatives')\n",
    "    \n",
    "    # Qualification scores distribution\n",
    "    plt.subplot(235)\n",
    "    qualification_scores = calculate_qualification_scores(credibility_matrix)\n",
    "    sns.histplot(qualification_scores, bins=20)\n",
    "    plt.title('Distribution of Qualification Scores')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed ranking information\n",
    "    print(\"\\nDetailed Ranking Information:\")\n",
    "    print(\"Top 10 Alternatives:\")\n",
    "    for rank, (alt_idx, score) in enumerate(final_ranking[:10], 1):\n",
    "        print(f\"Rank {rank}:\")\n",
    "        print(f\"  Alternative {alt_idx}\")\n",
    "        print(f\"  Score: {score:.3f}\")\n",
    "        print(\"  Criteria values:\")\n",
    "        for criterion in normalized_df.columns:\n",
    "            print(f\"    {criterion}: {normalized_df.iloc[alt_idx][criterion]:.3f}\")\n",
    "        print()\n",
    "    \n",
    "def calculate_qualification_scores(credibility_matrix, cutoff=0.6):\n",
    "    n = len(credibility_matrix)\n",
    "    qualification = np.zeros(n)\n",
    "    \n",
    "    for i in range(n):\n",
    "        strength = sum(1 for j in range(n) if credibility_matrix[i,j] > cutoff)\n",
    "        weakness = sum(1 for j in range(n) if credibility_matrix[j,i] > cutoff)\n",
    "        qualification[i] = strength - weakness\n",
    "    \n",
    "    return qualification\n",
    "\n",
    "visualize_electre_results(concordance_matrix, credibility_matrix, final_ranking, normalized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity Analysis\n",
    "def perform_sensitivity_analysis(base_weights, num_variations=10):\n",
    "    results = []\n",
    "    for _ in range(num_variations):\n",
    "        # Create variation in weights\n",
    "        variation = np.random.normal(0, 0.05, len(base_weights))  # 5% standard deviation\n",
    "        new_weights = {k: max(0.01, w + v) for (k, w), v in zip(base_weights.items(), variation)}\n",
    "        # Normalize to sum to 1\n",
    "        total = sum(new_weights.values())\n",
    "        new_weights = {k: w/total for k, w in new_weights.items()}\n",
    "        \n",
    "        # Recalculate everything with new weights\n",
    "        # (This would be a repeat of the main calculation with new weights)\n",
    "        # For brevity, we'll just store the weights and final ranking\n",
    "        results.append(new_weights)\n",
    "    \n",
    "    return results\n",
    "\n",
    "sensitivity_results = perform_sensitivity_analysis(weights)\n",
    "print(\"\\nSensitivity Analysis Results:\")\n",
    "print(\"Number of weight variations tested:\", len(sensitivity_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   import pandas as pd\n",
    "#   import numpy as np\n",
    "#   from sklearn import preprocessing\n",
    "#   import matplotlib.pyplot as plt\n",
    "#   import seaborn as sns\n",
    "#   \n",
    "#   # Your initial data preparation\n",
    "#   benefit_criteria = ['ABV', 'Ratings', 'Body']\n",
    "#   cost_criteria = ['Price', 'Acidity']\n",
    "#   all_criteria = benefit_criteria + cost_criteria\n",
    "#   \n",
    "#   # Create direction dictionary (1 for benefit/maximization, 0 for cost/minimization)\n",
    "#   direction = {}\n",
    "#   for criterion in benefit_criteria:\n",
    "#       direction[criterion] = 1\n",
    "#   for criterion in cost_criteria:\n",
    "#       direction[criterion] = 0\n",
    "#   \n",
    "#   # Copy the dataframe for normalization\n",
    "#   n_table = merged_df.copy()\n",
    "#   \n",
    "#   # Using normalization rule 2 (adapted from the original code)\n",
    "#   denom = dict(merged_df.apply(lambda x: x.max() - x.min()))\n",
    "#   _min = dict(merged_df.apply(lambda x: x.min()))\n",
    "#   _max = dict(merged_df.apply(lambda x: x.max()))\n",
    "#   \n",
    "#   print(_min)\n",
    "#   print(_max)\n",
    "#   \n",
    "#   # Normalize each column based on whether it's a benefit or cost criterion\n",
    "#   for column in merged_df.columns:\n",
    "#       if direction[column]:  # benefit criteria (maximize)\n",
    "#           n_table[column] = merged_df[column].apply(lambda x: (x - _min[column]) / denom[column])\n",
    "#       else:  # cost criteria (minimize)\n",
    "#           n_table[column] = merged_df[column].apply(lambda x: (_max[column] - x) / denom[column])\n",
    "#   \n",
    "#   _n_denom = dict(n_table.apply(lambda x: x.max() - x.min()))\n",
    "#   _n_min = dict(n_table.apply(lambda x: x.min()))\n",
    "#   _n_max = dict(n_table.apply(lambda x: x.max()))\n",
    "#   \n",
    "#   print(_n_denom)\n",
    "#   print(_n_min)\n",
    "#   print(_n_max)\n",
    "#   \n",
    "#   # Visualization of before and after normalization\n",
    "#   for column in merged_df.columns:\n",
    "#       plt.figure(figsize=(12, 4))\n",
    "#       plt.subplot(1, 2, 1)\n",
    "#       sns.kdeplot(merged_df[column])\n",
    "#       plt.title(f'Before normalization - {column}')\n",
    "#       plt.subplot(1, 2, 2)\n",
    "#       sns.kdeplot(n_table[column])\n",
    "#       plt.title(f'After normalization - {column}')\n",
    "#       plt.tight_layout()\n",
    "#       plt.show()\n",
    "#   \n",
    "#   # Define weights (equal weights for simplicity, can be modified)\n",
    "#   weights = {criterion: 1/len(all_criteria) for criterion in all_criteria}\n",
    "#   \n",
    "#   # Calculate concordance matrix\n",
    "#   c_matrix = pd.DataFrame(columns=n_table.index, index=n_table.index)\n",
    "#   \n",
    "#   for option in n_table.index:\n",
    "#       for option2 in n_table.index:\n",
    "#           _sum = 0\n",
    "#           for criterion in n_table.columns:\n",
    "#               if n_table.loc[option, criterion] > n_table.loc[option2, criterion]:\n",
    "#                   _sum += weights[criterion]\n",
    "#               elif np.isclose(n_table.loc[option, criterion], n_table.loc[option2, criterion]):\n",
    "#                   _sum += 0.5 * weights[criterion]\n",
    "#           if option == option2:\n",
    "#               c_matrix.loc[option, option2] = 0\n",
    "#           else:\n",
    "#               c_matrix.loc[option, option2] = _sum\n",
    "#   \n",
    "#   # Calculate discordance matrix\n",
    "#   d_matrix = pd.DataFrame(columns=n_table.index, index=n_table.index)\n",
    "#   \n",
    "#   for option in n_table.index:\n",
    "#       for option2 in n_table.index:\n",
    "#           diffs = list(n_table.loc[option, :] - n_table.loc[option2, :])\n",
    "#           if not any(diffs):\n",
    "#               _discordance_index = 0\n",
    "#           else:\n",
    "#               n_diffs = [x for x in diffs if x < 0]\n",
    "#               if not n_diffs:\n",
    "#                   num = 0\n",
    "#               else:\n",
    "#                   num = max(np.abs(n_diffs))\n",
    "#               denom = max(np.abs(diffs))\n",
    "#               _discordance_index = num / denom\n",
    "#           d_matrix.loc[option, option2] = _discordance_index\n",
    "#   \n",
    "#   # Calculate aggregated dominance matrix\n",
    "#   a_matrix = pd.DataFrame(columns=c_matrix.columns, index=c_matrix.index)\n",
    "#   \n",
    "#   # Define thresholds (can be modified)\n",
    "#   concordance_threshold = 0.7\n",
    "#   discordance_threshold = 0.3\n",
    "#   \n",
    "#   for option in c_matrix.columns:\n",
    "#       for option2 in c_matrix.index:\n",
    "#           a_matrix.loc[option, option2] = 1 if (c_matrix.loc[option, option2] >= concordance_threshold and \n",
    "#                                               d_matrix.loc[option, option2] <= discordance_threshold) else 0\n",
    "#   \n",
    "#   # Calculate net dominance scores\n",
    "#   dominance_scores = a_matrix.sum(axis=1) - a_matrix.sum(axis=0)\n",
    "#   rankings = dominance_scores.sort_values(ascending=False)\n",
    "#   \n",
    "#   print(\"\\nFinal Rankings:\")\n",
    "#   print(rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   credibility_matrix = np.zeros((num_alternatives, num_alternatives))\n",
    "#   \n",
    "#   for i in range(num_alternatives):\n",
    "#       for j in range(num_alternatives):\n",
    "#           if i != j:\n",
    "#               if discordance_matrix[i][j] > concordance_matrix[i][j]:\n",
    "#                   credibility_matrix[i][j] = concordance_matrix[i][j] * \\\n",
    "#                       (1 - discordance_matrix[i][j]) / (1 - concordance_matrix[i][j])\n",
    "#               else:\n",
    "#                   credibility_matrix[i][j] = concordance_matrix[i][j]\n",
    "#   \n",
    "#   credibility_df = pd.DataFrame(\n",
    "#       credibility_matrix,\n",
    "#       index=alternatives,\n",
    "#       columns=alternatives\n",
    "#   )\n",
    "#   print(\"\\nCredibility Matrix:\")\n",
    "#   print(credibility_df)\n",
    "#   \n",
    "#   # Step 4: Calculate Net Credibility Scores\n",
    "#   net_scores = np.zeros(num_alternatives)\n",
    "#   \n",
    "#   for i in range(num_alternatives):\n",
    "#       outgoing = np.sum(credibility_matrix[i, :])\n",
    "#       incoming = np.sum(credibility_matrix[:, i])\n",
    "#       net_scores[i] = outgoing - incoming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Heatmap of Credibility Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(credibility_df, annot=True, cmap='YlOrRd', center=0)\n",
    "plt.title('Credibility Matrix Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Bar plot of Net Scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(alternatives, net_scores)\n",
    "plt.title('Net Credibility Scores by Alternative')\n",
    "plt.xlabel('Alternatives')\n",
    "plt.ylabel('Net Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Rankings visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(rankings.index, rankings.values)\n",
    "plt.title('Alternatives Ranked by Net Credibility Score')\n",
    "plt.xlabel('Alternatives')\n",
    "plt.ylabel('Net Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
