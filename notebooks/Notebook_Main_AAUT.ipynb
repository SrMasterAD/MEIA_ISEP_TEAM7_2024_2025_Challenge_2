{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inicialização dataset - Task [001]"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ftfy import fix_text\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from category_encoders import BinaryEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# open the csv file with pandas and read it into a dataframe\n",
    "df = pd.read_csv('../datasets/WineDataset.csv')\n",
    "df2 = pd.read_csv('../datasets/XWines_Full_100K_wines.csv')\n",
    "df3 = pd.read_csv('../datasets/merged_wine_dataset.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print all the information about the dataframe\n",
    "df.info()\n",
    "df.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print all the information about the dataframe\n",
    "df2.info()\n",
    "df2.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print all the information about the dataframe\n",
    "df3.info()\n",
    "df3.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# fix text using ftfy\n",
    "# fix all the columns except when the column is a float\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = df[col].apply(lambda x: fix_text(x) if isinstance(x, str) else x)\n",
    "\n",
    "# fix text using ftfy\n",
    "# fix all the columns except when the column is a float\n",
    "for col in df2.columns:\n",
    "    if df2[col].dtype == 'object':\n",
    "        df2[col] = df2[col].apply(lambda x: fix_text(x) if isinstance(x, str) else x)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# view all the unique values for each column except for the columns:'Title' 'Description', 'Country', 'Unit' 'Region' 'Appellation'\n",
    "for col in df.columns:\n",
    "    if col not in ['Title', 'Description', 'Country', 'Unit', 'Region', 'Appellation']:\n",
    "        print(col, df[col].unique())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for col in df3.columns:\n",
    "    if col not in ['WineName', 'WineryName', 'Grape', 'Secondary Grape Varieties', 'Country', 'Region', 'Appellation', 'Style', 'Characteristics', 'Description']:\n",
    "        print(col, df3[col].unique())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Limpeza de dados\n",
    "- Remoção de valores nulos\n",
    "- Normalização de valores\n",
    "- Remoção de colunas desnecessárias"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print wine where wineid 131027\n",
    "print(df2.loc[df2['WineID'] == 131027])\n",
    "\n",
    "#One-Hot Encoding for the column 'Type' 'Elaborate' 'Body', 'Acidity'\n",
    "\n",
    "one_hot_encoded_df = pd.get_dummies(df2, columns=['Type','Body','Acidity','Elaborate'], prefix=['Type','Body','Acidity','Elaborate'])\n",
    "\n",
    "# Binary Encoding for the column 'Country', 'RegionName', 'Grapes', 'Harmonize'\n",
    "\n",
    "encoder = BinaryEncoder(cols=['Grapes', 'Harmonize'], return_df=True)\n",
    "binary_encoded_df = encoder.fit_transform(df2)\n",
    "\n",
    "encoded_df = pd.concat([one_hot_encoded_df, binary_encoded_df], axis=1)\n",
    "\n",
    "# Normalize the 'ABV' column using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalizing the 'ABV' column\n",
    "encoded_df['ABV'] = scaler.fit_transform(encoded_df[['ABV']])\n",
    "\n",
    "\n",
    "encoded_df = encoded_df.drop(columns=df2.columns)\n",
    "encoded_df = encoded_df.dropna()\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(encoded_df)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Classification Dataset Preprocessing - Task [066]\n",
    "#### Normalizing Harmonize and grouping it into broader categories"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Count how many grapes in the column grapes are unique grapes are in a array\n",
    "grapes = df2['Grapes'].str.split(',').explode().unique()\n",
    "\n",
    "#Remove every \"]\"\" and \"[\" and \",\" and \"'\" and remove spaces if they are in the first character or the last character\n",
    "grapes = [grape.replace(']', '').replace('[', '').replace(',', '').replace(\"'\", '').strip() for grape in grapes]\n",
    "\n",
    "grapes2 = ['Tempranillo', 'Chardonnay', 'Sauvignon Blanc', 'Pinot Noir', 'Glera', 'Chenin Blanc', 'Castelão ', 'Malagousia', 'Cinsault', 'Grenache', 'Shiraz', 'Cabernet Sauvignon', 'Bacchus', 'Viognier', 'Pinot Grigio', 'Garnacha', 'Malbec', 'Cortese', 'Merlot', 'Melon De Bourgogne', 'Carménère', 'Zinfandel', 'Syrah', 'Marsanne', 'Gruner Veltliner', 'Corvina', 'Greco', 'Macabeo', 'Gamay', 'Loureiro', 'Riesling', 'Alvarinho', 'Mourvèdre', 'Cabernet Franc', 'Vespaiola', 'Picpoul', 'Vermentino', 'Sangiovese', 'Pinot Meunier', 'Verdejo', 'Primitivo', 'Pinotage', 'Alicante Bouschet', 'Garganega', 'Godello', 'Carignan', 'Grenache Blanc', 'Aligoté', 'Siegerrebe', 'Touriga Nacional', 'Albarino', 'Nerello Mascalese', \"Nero D'Avola\", 'Turbiana', 'Pinot Gris', 'Airen', 'Trincadeira', 'Tinta Roriz', 'Xinomavro', 'Agiorgitiko', 'Pais', 'Gewürztraminer', 'Mencia', 'Verdicchio', 'Fiano', 'Rondinella', 'Mauzac', 'Nebbiolo', 'Lambrusco Grasparossa', 'Pecorino', 'Negroamaro', 'Fernão Pires', 'Feteasca Alba', 'Zwieigelt', 'Grillo', 'Tinta Barroca', 'Furmint', 'Touriga Franca', 'Pedro Ximénez', 'Palomino', 'Viura', 'Montepulciano', 'Barbera', 'Malvasia', 'Arinto', 'Colombard', 'Sémillon', 'Jacquere', 'Pinot Blanc', 'Rkatsiteli', 'Syrah-Shiraz', 'Torrontes', 'Sauvignon Gris', 'Assyrtiko', 'Moscato', 'Rolle', 'Huxelrebe', 'Xarel-Lo', 'Niellucciu', 'Saperavi', 'Rara Neagra', 'Non Varietal', 'Muscat', 'Monastrell', 'Dolcetto', 'Black Muscat', 'Nerello', 'Roussanne', 'Tibouren', 'Grolleau', 'Inzolia', 'Falanghina', 'Aglianico']\n",
    "\n",
    "# Create a new array of grapes where both of the grapes match\n",
    "grapes = list(set(grapes) & set(grapes2))\n",
    "print(grapes)\n",
    "\n",
    "df2['Grapes'] = df2['Grapes'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "df2['Grapes'] = df2['Grapes'].apply(\n",
    "    lambda x: [grape for grape in x if grape in grapes]\n",
    ")\n",
    "\n",
    "# Drop rows where the resulting 'Grapes' column is empty\n",
    "df2 = df2[df2['Grapes'].str.len() > 0]\n",
    "\n",
    "df2['Grapes'] = df2['Grapes'].apply(\n",
    "    lambda x: x[0]\n",
    ")\n",
    "\n",
    "# Replace dishes with their broader categories in the Harmonize column\n",
    "df2['Harmonize'] = df2['Harmonize'].apply(eval)\n",
    "dish_to_category = {\n",
    "    'Beef': 'Meat', 'Lamb': 'Meat', 'Pork': 'Meat', 'Veal': 'Meat', 'Game Meat': 'Meat',\n",
    "    'Duck': 'Meat', 'Ham': 'Meat', 'Cold Cuts': 'Meat', 'Cured Meat': 'Meat',\n",
    "    'Poultry': 'Poultry', 'Chicken': 'Poultry',\n",
    "    'Rich Fish': 'Fish & Seafood', 'Lean Fish': 'Fish & Seafood', 'Shellfish': 'Fish & Seafood',\n",
    "    'Seafood': 'Fish & Seafood', 'Sushi': 'Fish & Seafood', 'Sashimi': 'Fish & Seafood',\n",
    "    'Codfish': 'Fish & Seafood', 'Fish': 'Fish & Seafood', 'Grilled': 'Fish & Seafood',\n",
    "    'Soft Cheese': 'Cheese', 'Hard Cheese': 'Cheese', 'Blue Cheese': 'Cheese',\n",
    "    'Maturated Cheese': 'Cheese', 'Goat Cheese': 'Cheese', 'Mild Cheese': 'Cheese',\n",
    "    'Medium-cured Cheese': 'Cheese', 'Cheese': 'Cheese',\n",
    "    'Pasta': 'Pasta', 'Tagliatelle': 'Pasta', 'Lasagna': 'Pasta',\n",
    "    'Paella': 'Fish & Seafood', 'Pizza' : 'Pasta',\n",
    "    'Vegetarian': 'Vegetarian & Vegan', 'Mushrooms': 'Vegetarian & Vegan', 'Salad': 'Vegetarian & Vegan',\n",
    "    'Fruit': 'Vegetarian & Vegan', 'Tomato Dishes': 'Vegetarian & Vegan', 'Beans': 'Vegetarian & Vegan',\n",
    "    'Eggplant Parmigiana': 'Vegetarian & Vegan', 'Light Stews': 'Vegetarian & Vegan',\n",
    "    'Appetizer': 'Appetizers & Snacks', 'Snack': 'Appetizers & Snacks',\n",
    "    'Aperitif': 'Appetizers & Snacks', 'French Fries': 'Appetizers & Snacks', 'Baked Potato': 'Appetizers & Snacks',\n",
    "    'Cream': 'Appetizers & Snacks',\n",
    "    'Sweet Dessert': 'Desserts', 'Fruit Dessert': 'Desserts', 'Citric Dessert': 'Desserts',\n",
    "    'Cake': 'Desserts', 'Chocolate': 'Desserts', 'Cookies': 'Desserts',\n",
    "    'Chestnut': 'Desserts', 'Spiced Fruit Cake': 'Desserts', 'Dessert': 'Desserts',\n",
    "    'Soufflé': 'Desserts', 'Dried Fruits': 'Desserts',\n",
    "    'Spicy Food': 'Spicy Food', 'Curry Chicken': 'Spicy Food', 'Asian Food': 'Spicy Food', 'Yakissoba': 'Spicy Food',\n",
    "    'Barbecue': 'Meat', 'Roast': 'Meat'\n",
    "}\n",
    "\n",
    "columns = df2.columns\n",
    "new_harmonize = df2.copy()\n",
    "new_harmonize['Harmonize'] = df2['Harmonize'].apply(lambda x: list(set(dish_to_category.get(dish, dish) for dish in x)))\n",
    "\n",
    "# Expand the Harmonize column into multiple rows, one for each dish\n",
    "new_harmonize = new_harmonize.explode('Harmonize')\n",
    "\n",
    "# Reset the index for consistency and remove any rows with 'Risotto' because it has a number of insignificant occurrences\n",
    "new_harmonize.reset_index(drop=True, inplace=True)\n",
    "new_harmonize = new_harmonize[new_harmonize['Harmonize'].apply(lambda x: 'Risotto' not in x)]\n",
    "df2 = new_harmonize.copy()\n",
    "\n",
    "# Get the unique dishes\n",
    "unique_categories = df2['Harmonize'].unique()\n",
    "\n",
    "# One-hot encoding\n",
    "for dish in unique_categories:\n",
    "    new_harmonize[f'Harmonize_{dish}'] = new_harmonize['Harmonize'].apply(lambda x: x == dish)\n",
    "# Turn new_harmonize into a harmonize dataframe\n",
    "new_harmonize = new_harmonize.drop(columns=columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Normalizing the rest of the columns"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#One-Hot Encoding for the column 'Type' 'Elaborate' 'Body', 'Acidity', 'Grapes'\n",
    "one_hot_encoded_df = pd.get_dummies(df2, columns=['Type', 'Elaborate', 'Body', 'Acidity', 'Grapes'])\n",
    "\n",
    "encoded_df = one_hot_encoded_df.copy()\n",
    "\n",
    "# Maximum ABV value\n",
    "print(encoded_df['ABV'].min())\n",
    "print(encoded_df['ABV'].max())\n",
    "\n",
    "# Normalizing the 'ABV' column\n",
    "scaler = MinMaxScaler()\n",
    "encoded_df['ABV'] = scaler.fit_transform(encoded_df[['ABV']])\n",
    "\n",
    "# Dropping unnecessary columns\n",
    "encoded_df = encoded_df.drop(columns=['Country', 'RegionName', 'Code', 'WineName', 'WineID', 'Vintages', 'Website', 'WineryID', 'WineryName', 'RegionID','Harmonize'])\n",
    "encoded_df = encoded_df.dropna()\n",
    "print(encoded_df.columns)\n",
    "# add new_harmonize to the encoded_df\n",
    "encoded_df = pd.concat([encoded_df, new_harmonize], axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Boxplot"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate the IQR\n",
    "numeric_df = encoded_df['ABV']\n",
    "q1 = numeric_df.quantile(0.25)\n",
    "q3 = numeric_df.quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "# Boxplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=numeric_df, showmeans=True, orient=\"h\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Analyzing the resulting boxplot, we observe that some values fall outside the ranges but are still relevant, as 0% alcohol wines, including dealcoholized varieties, remain within the wine category. These options retain the essence of traditional wines without alcohol. Additionally, beverages like firewater, with higher alcohol content, can also bring value to the project."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Correlation Matrix"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute the correlation matrix without the Elaborate columns to reduce the size of the heatmap\n",
    "drop_columns = encoded_df.columns[encoded_df.columns.str.startswith('Elaborate_')]\n",
    "drop_columns = drop_columns.append(encoded_df.columns[encoded_df.columns.str.startswith('Grapes_')])\n",
    "matrix_df = encoded_df.drop(columns=drop_columns)\n",
    "correlation_matrix = matrix_df.corr()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    cbar=True,\n",
    "    annot_kws={\"size\": 10}\n",
    ")\n",
    "plt.xticks(fontsize=12, rotation=45)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Analyzing the matrix, we observe that it is relatively ‘cold,’ meaning it exhibits a low correlation index overall. Despite the generally low correlations between features, certain specific relationships do stand out. For instance, some dishes in the ‘Harmonize’ category show a notable correlation with particular wines, while certain body characteristics correlate with the alcohol by volume (ABV) and wine types."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### PCA"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "harmonize_columns = [col for col in encoded_df.columns if col.startswith('Harmonize_')]\n",
    "\n",
    "# Loop through each 'harmonize_' column and perform PCA\n",
    "for label_col in harmonize_columns:\n",
    "    print(f\"Generating PCA plot for label column: {label_col}\")\n",
    "\n",
    "    features = encoded_df.drop(columns=[label_col])\n",
    "    labels = encoded_df[label_col].values\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    df_pca = pca.fit_transform(features)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    unique_labels = np.unique(labels)\n",
    "    for label in unique_labels:\n",
    "        plt.scatter(\n",
    "            df_pca[labels == label, 0],\n",
    "            df_pca[labels == label, 1],\n",
    "            label=f'Classe {label}'\n",
    "        )\n",
    "\n",
    "    plt.title(f'PCA - Column: {label_col}')\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "When applying PCA to identify patterns and relationships among the different categories in the ‘Harmonize’ column, we encountered inconclusive results. The resulting plots showed the various categories exhibiting similar patterns, with data points widely scattered across the graphical space rather than concentrated in specific zones."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Neural Network"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 1: Assume the data (encoded_df) is preprocessed externally\n",
    "# Ensure relevant columns are present in encoded_df\n",
    "columns_needed = [\n",
    "    'ABV', 'Harmonize_Meat', 'Harmonize_Poultry', 'Harmonize_Cheese', 'Harmonize_Spicy Food',\n",
    "    'Harmonize_Appetizers & Snacks', 'Harmonize_Fish & Seafood', 'Harmonize_Vegetarian & Vegan',\n",
    "    'Harmonize_Desserts', 'Harmonize_Pasta'\n",
    "]  # Add other relevant columns as needed\n",
    "assert all(col in encoded_df.columns for col in columns_needed), \"Required columns are missing from encoded_df\"\n",
    "\n",
    "# Normalize numerical features (e.g., 'ABV')\n",
    "scaler = MinMaxScaler()\n",
    "numeric_columns = ['ABV']\n",
    "encoded_df[numeric_columns] = scaler.fit_transform(encoded_df[numeric_columns])\n",
    "\n",
    "# Step 2: Feature and Target Preparation\n",
    "harmonize_columns = [col for col in encoded_df.columns if col.startswith('Harmonize_')]\n",
    "type_columns = [col for col in encoded_df.columns if col.startswith('Type_')]\n",
    "grape_columns = [col for col in encoded_df.columns if col.startswith('Grapes_')]\n",
    "\n",
    "features = encoded_df.drop(columns=harmonize_columns)  # Features exclude harmonize columns\n",
    "labels = encoded_df[harmonize_columns]  # Targets are harmonize columns\n",
    "\n",
    "# Step 3: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Neural Network Setup\n",
    "model = Sequential([\n",
    "    Dense(units=y_train.shape[1], input_dim=X_train.shape[1], activation='softmax')  # Single-layer with softmax\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 5: Train the Model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Step 6: Evaluate the Model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Step 7: Visualize Training Progress\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Classification Model Evaluation"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Predictions as classification for a single meal"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def predict_for_dish(dish_name, encoded_df, model):\n",
    "    # Verify the dish exists in the Harmonize mapping\n",
    "    harmonize_columns = [col for col in encoded_df.columns if col.startswith(\"Harmonize_\")]\n",
    "    harmonize_mapping = {col.replace(\"Harmonize_\", \"\"): col for col in harmonize_columns}\n",
    "\n",
    "    if dish_name not in harmonize_mapping:\n",
    "        raise ValueError(f\"Dish '{dish_name}' is not in the Harmonize categories.\")\n",
    "\n",
    "    # Create a custom input vector\n",
    "    custom_features = encoded_df.iloc[0].copy()  # Start with a template row\n",
    "    custom_features[:] = 0  # Set all values to 0 (neutral baseline)\n",
    "    custom_features[harmonize_mapping[dish_name]] = 1  # Set the specific Harmonize column to 1\n",
    "\n",
    "    # Ensure numerical features like 'ABV' are filled with a reasonable default\n",
    "    if 'ABV' in custom_features.index:\n",
    "        custom_features['ABV'] = 0.5  # Default normalized value (e.g., midpoint)\n",
    "\n",
    "    # Drop harmonize columns to match input features for the model\n",
    "    custom_features = custom_features.drop(harmonize_columns)\n",
    "\n",
    "    # Convert to numeric format (NumPy array)\n",
    "    custom_features = custom_features.values.astype(float).reshape(1, -1)\n",
    "\n",
    "    # Predict using the trained model\n",
    "    prediction = model.predict(custom_features)\n",
    "    predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "\n",
    "    # Map prediction back to Harmonize, Wine Types, and Grapes\n",
    "    predicted_harmonize_label = list(harmonize_mapping.keys())[predicted_class]\n",
    "    matching_rows = encoded_df[encoded_df[harmonize_mapping[predicted_harmonize_label]] == 1]\n",
    "\n",
    "    # Get wine types and grape types\n",
    "    type_columns = [col for col in encoded_df.columns if col.startswith('Type_')]\n",
    "    grape_columns = [col for col in encoded_df.columns if col.startswith('Grapes_')]\n",
    "    wine_types = matching_rows[type_columns].idxmax(axis=1).str.replace(\"Type_\", \"\").tolist()\n",
    "    grapes = matching_rows[grape_columns].idxmax(axis=1).str.replace(\"Grapes_\", \"\").tolist()\n",
    "\n",
    "    # Return the prediction result\n",
    "    return {\"Harmonize\": predicted_harmonize_label, \"Wine Types\": wine_types, \"Grapes\": grapes}\n",
    "\n",
    "# Example usage:\n",
    "dish_name = \"Meat\"  # Replace with the dish you want to predict for\n",
    "result = predict_for_dish(dish_name, encoded_df, model)\n",
    "\n",
    "print(f\"Prediction for dish '{dish_name}':\")\n",
    "print(f\"Harmonize: {result['Harmonize']}\")\n",
    "print(f\"Wine Types: {result['Wine Types']}\")\n",
    "print(f\"Grapes: {result['Grapes']}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Classification for a Subset of Test Data (e.g., 10 items)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Predictions for a Subset of Test Data (e.g., 10 items)\n",
    "predictions = model.predict(X_test[:10])  # Predict only for the first 10 samples\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "y_true_classes = np.argmax(y_test[:10].values, axis=1)\n",
    "\n",
    "# Restore Harmonize mapping\n",
    "harmonize_mapping = {col: col.replace(\"Harmonize_\", \"\") for col in harmonize_columns}\n",
    "predicted_harmonize_labels = [harmonize_mapping[harmonize_columns[pred]] for pred in predicted_classes]\n",
    "\n",
    "# Map predictions back to wines and grapes (simplified for available columns)\n",
    "results = []\n",
    "for harmonize_label in predicted_harmonize_labels:\n",
    "    matching_rows = encoded_df[encoded_df[f'Harmonize_{harmonize_label}'] == 1]\n",
    "    wines = matching_rows.index.tolist()  # Assuming wines are represented by row indices or other identifiers\n",
    "    grapes = matching_rows.filter(like='Grapes_').idxmax(axis=1).tolist()  # Get most likely grape columns\n",
    "    results.append({\"Harmonize\": harmonize_label, \"Wines\": wines, \"Grapes\": grapes})\n",
    "\n",
    "# Print results for each test sample\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Test Sample {i + 1}:\")\n",
    "    print(f\"Harmonize: {result['Harmonize']}\")\n",
    "    print(f\"Wines: {result['Wines']}\")\n",
    "    print(f\"Grapes: {result['Grapes']}\")\n",
    "    print(\"-\" * 40)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Predictions for the Entire Test Data"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Predictions for the Entire Test Data\n",
    "predictions = model.predict(X_test)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "y_true_classes = np.argmax(y_test.values, axis=1)\n",
    "\n",
    "# Restore Harmonize mapping\n",
    "harmonize_mapping = {col: col.replace(\"Harmonize_\", \"\") for col in harmonize_columns}\n",
    "predicted_harmonize_labels = [harmonize_mapping[harmonize_columns[pred]] for pred in predicted_classes]\n",
    "\n",
    "# Map predictions back to wines and grapes (simplified for available columns)\n",
    "results = []\n",
    "for harmonize_label in predicted_harmonize_labels:\n",
    "    matching_rows = encoded_df[encoded_df[f'Harmonize_{harmonize_label}'] == 1]\n",
    "    wines = matching_rows.index.tolist()  # Assuming wines are represented by row indices or other identifiers\n",
    "    grapes = matching_rows.filter(like='Grapes_').idxmax(axis=1).tolist()  # Get most likely grape columns\n",
    "    results.append({\"Harmonize\": harmonize_label, \"Wines\": wines, \"Grapes\": grapes})\n",
    "\n",
    "# Print results for each test sample\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Test Sample {i + 1}:\")\n",
    "    print(f\"Harmonize: {result['Harmonize']}\")\n",
    "    print(f\"Wines: {result['Wines']}\")\n",
    "    print(f\"Grapes: {result['Grapes']}\")\n",
    "    print(\"-\" * 40)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Measures"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "ROC Curve: Plots true positive rate vs. false positive rate.\n",
    "AUC: Measures the ability of the model to distinguish between classes."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Calculate ROC-AUC for each valid class\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, class_name in enumerate(list(harmonize_mapping.values())):\n",
    "    # Check if the class has more than one unique value\n",
    "    if len(np.unique(y_test[:10].values[:, i])) > 1:\n",
    "        fpr, tpr, _ = roc_curve(y_test[:10].values[:, i], predictions[:, i])\n",
    "        auc_score = roc_auc_score(y_test[:10].values[:, i], predictions[:, i])\n",
    "        plt.plot(fpr, tpr, label=f'{class_name} (AUC: {auc_score:.2f})')\n",
    "    else:\n",
    "        print(f\"Skipping ROC-AUC for class '{class_name}' due to lack of variation in y_test.\")\n",
    "\n",
    "# Plot formatting\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for reference\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Confusion Matrix\n",
    "Provides a detailed breakdown of true positives, true negatives, false positives, and false negatives."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_true_classes, predicted_classes)\n",
    "\n",
    "# Extract the actual class labels\n",
    "unique_labels = sorted(np.unique(np.concatenate((y_true_classes, predicted_classes))))\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[harmonize_mapping[harmonize_columns[i]] for i in unique_labels])\n",
    "disp.plot(cmap='viridis', xticks_rotation='vertical')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy.stats import binomtest\n",
    "\n",
    "# Compare model accuracy to random guessing (baseline)\n",
    "random_accuracy = 1 / len(harmonize_columns)  # Assume equal probability for each class\n",
    "correct_predictions = np.sum(predicted_classes == y_true_classes)\n",
    "total_predictions = len(y_true_classes)\n",
    "\n",
    "# Perform binomial test\n",
    "binom_test_result = binomtest(correct_predictions, total_predictions, random_accuracy)\n",
    "p_value = binom_test_result.pvalue\n",
    "\n",
    "print(f\"Model Accuracy: {correct_predictions / total_predictions:.4f}, P-Value: {p_value:.4f}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"Model predictions are significantly better than random guessing.\")\n",
    "else:\n",
    "    print(\"Model predictions are not significantly better than random guessing.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
