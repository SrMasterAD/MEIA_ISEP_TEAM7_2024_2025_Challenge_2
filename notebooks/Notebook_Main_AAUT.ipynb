{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicialização dataset - Task [001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ftfy import fix_text\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from category_encoders import BinaryEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the csv file with pandas and read it into a dataframe\n",
    "df = pd.read_csv('../datasets/WineDataset.csv')\n",
    "df2 = pd.read_csv('../datasets/XWines_Full_100K_wines.csv')\n",
    "df3 = pd.read_csv('../datasets/merged_wine_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all the information about the dataframe\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all the information about the dataframe\n",
    "df2.info()\n",
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all the information about the dataframe\n",
    "df3.info()\n",
    "df3.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix text using ftfy\n",
    "# fix all the columns except when the column is a float\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object': \n",
    "        df[col] = df[col].apply(lambda x: fix_text(x) if isinstance(x, str) else x)\n",
    "\n",
    "# fix text using ftfy\n",
    "# fix all the columns except when the column is a float\n",
    "for col in df2.columns:\n",
    "    if df2[col].dtype == 'object': \n",
    "        df2[col] = df2[col].apply(lambda x: fix_text(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view all the unique values for each column except for the columns:'Title' 'Description', 'Country', 'Unit' 'Region' 'Appellation'\n",
    "for col in df.columns:\n",
    "    if col not in ['Title', 'Description', 'Country', 'Unit', 'Region', 'Appellation']:\n",
    "        print(col, df[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df3.columns:\n",
    "    if col not in ['WineName', 'WineryName', 'Grape', 'Secondary Grape Varieties', 'Country', 'Region', 'Appellation', 'Style', 'Characteristics', 'Description']:\n",
    "        print(col, df3[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpeza de dados\n",
    "- Remoção de valores nulos\n",
    "- Normalização de valores\n",
    "- Remoção de colunas desnecessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print wine where wineid 131027\n",
    "print(df2.loc[df2['WineID'] == 131027])\n",
    "\n",
    "#One-Hot Encoding for the column 'Type' 'Elaborate' 'Body', 'Acidity'\n",
    "\n",
    "one_hot_encoded_df = pd.get_dummies(df2, columns=['Type','Body','Acidity','Elaborate'], prefix=['Type','Body','Acidity','Elaborate'])\n",
    "\n",
    "# Binary Encoding for the column 'Country', 'RegionName', 'Grapes', 'Harmonize'\n",
    "\n",
    "encoder = BinaryEncoder(cols=['Grapes', 'Harmonize'], return_df=True)\n",
    "binary_encoded_df = encoder.fit_transform(df2)\n",
    "\n",
    "encoded_df = pd.concat([one_hot_encoded_df, binary_encoded_df], axis=1)\n",
    "\n",
    "# Normalize the 'ABV' column using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalizing the 'ABV' column\n",
    "encoded_df['ABV'] = scaler.fit_transform(encoded_df[['ABV']])\n",
    "\n",
    "\n",
    "encoded_df = encoded_df.drop(columns=df2.columns)\t\n",
    "encoded_df = encoded_df.dropna()\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(encoded_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Dataset Preprocessing - Task [066]\n",
    "### Normalizing Harmonize and grouping it into broader categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace dishes with their broader categories in the Harmonize column\n",
    "df2['Harmonize'] = df2['Harmonize'].apply(eval)\n",
    "dish_to_category = {\n",
    "    'Beef': 'Meat', 'Lamb': 'Meat', 'Pork': 'Meat', 'Veal': 'Meat', 'Game Meat': 'Meat',\n",
    "    'Duck': 'Meat', 'Ham': 'Meat', 'Cold Cuts': 'Meat', 'Cured Meat': 'Meat',\n",
    "    'Poultry': 'Poultry', 'Chicken': 'Poultry',\n",
    "    'Rich Fish': 'Fish & Seafood', 'Lean Fish': 'Fish & Seafood', 'Shellfish': 'Fish & Seafood',\n",
    "    'Seafood': 'Fish & Seafood', 'Sushi': 'Fish & Seafood', 'Sashimi': 'Fish & Seafood',\n",
    "    'Codfish': 'Fish & Seafood', 'Fish': 'Fish & Seafood', 'Grilled': 'Fish & Seafood',\n",
    "    'Soft Cheese': 'Cheese', 'Hard Cheese': 'Cheese', 'Blue Cheese': 'Cheese',\n",
    "    'Maturated Cheese': 'Cheese', 'Goat Cheese': 'Cheese', 'Mild Cheese': 'Cheese',\n",
    "    'Medium-cured Cheese': 'Cheese', 'Cheese': 'Cheese',\n",
    "    'Pasta': 'Pasta', 'Tagliatelle': 'Pasta', 'Lasagna': 'Pasta',\n",
    "    'Paella': 'Fish & Seafood', 'Pizza' : 'Pasta',\n",
    "    'Vegetarian': 'Vegetarian & Vegan', 'Mushrooms': 'Vegetarian & Vegan', 'Salad': 'Vegetarian & Vegan',\n",
    "    'Fruit': 'Vegetarian & Vegan', 'Tomato Dishes': 'Vegetarian & Vegan', 'Beans': 'Vegetarian & Vegan',\n",
    "    'Eggplant Parmigiana': 'Vegetarian & Vegan', 'Light Stews': 'Vegetarian & Vegan',\n",
    "    'Appetizer': 'Appetizers & Snacks', 'Snack': 'Appetizers & Snacks',\n",
    "    'Aperitif': 'Appetizers & Snacks', 'French Fries': 'Appetizers & Snacks', 'Baked Potato': 'Appetizers & Snacks',\n",
    "    'Cream': 'Appetizers & Snacks',\n",
    "    'Sweet Dessert': 'Desserts', 'Fruit Dessert': 'Desserts', 'Citric Dessert': 'Desserts',\n",
    "    'Cake': 'Desserts', 'Chocolate': 'Desserts', 'Cookies': 'Desserts',\n",
    "    'Chestnut': 'Desserts', 'Spiced Fruit Cake': 'Desserts', 'Dessert': 'Desserts',\n",
    "    'Soufflé': 'Desserts', 'Dried Fruits': 'Desserts',\n",
    "    'Spicy Food': 'Spicy Food', 'Curry Chicken': 'Spicy Food', 'Asian Food': 'Spicy Food', 'Yakissoba': 'Spicy Food',\n",
    "    'Barbecue': 'Meat', 'Roast': 'Meat'\n",
    "}\n",
    "\n",
    "columns = df2.columns\n",
    "new_harmonize = df2.copy()\n",
    "new_harmonize['Harmonize'] = df2['Harmonize'].apply(lambda x: list(set(dish_to_category.get(dish, dish) for dish in x)))\n",
    "\n",
    "# Expand the Harmonize column into multiple rows, one for each dish\n",
    "new_harmonize = new_harmonize.explode('Harmonize')\n",
    "\n",
    "# Reset the index for consistency and remove any rows with 'Risotto' because it has a number of insignificant occurrences\n",
    "new_harmonize.reset_index(drop=True, inplace=True)\n",
    "new_harmonize = new_harmonize[new_harmonize['Harmonize'].apply(lambda x: 'Risotto' not in x)]\n",
    "df2 = new_harmonize.copy()\n",
    "\n",
    "# Get the unique dishes\n",
    "unique_categories = df2['Harmonize'].unique()\n",
    "\n",
    "# One-hot encoding\n",
    "for dish in unique_categories:\n",
    "    new_harmonize[f'Harmonize_{dish}'] = new_harmonize['Harmonize'].apply(lambda x: x == dish)\n",
    "# Turn new_harmonize into a harmonize dataframe\n",
    "new_harmonize = new_harmonize.drop(columns=columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the rest of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-Hot Encoding for the column 'Type' 'Elaborate' 'Body', 'Acidity'\n",
    "one_hot_encoded_df = pd.get_dummies(df2, columns=['Type', 'Elaborate', 'Body', 'Acidity'])\n",
    "\n",
    "# Binary Encoding for the column 'Grapes'\n",
    "encoder = BinaryEncoder(cols=['Grapes'], return_df=True)\n",
    "binary_encoded_df = encoder.fit_transform(df2)\n",
    "\n",
    "encoded_df = one_hot_encoded_df.copy()\n",
    "\n",
    "# Maximum ABV value\n",
    "print(encoded_df['ABV'].min())\n",
    "print(encoded_df['ABV'].max())\n",
    "\n",
    "# get the row where the abv is min\n",
    "print(encoded_df[encoded_df['ABV'] == encoded_df['ABV'].min()])\n",
    "\n",
    "\n",
    "# Normalizing the 'ABV' column\n",
    "scaler = MinMaxScaler()\n",
    "encoded_df['ABV'] = scaler.fit_transform(encoded_df[['ABV']])\n",
    "\n",
    "\n",
    "# Dropping unnecessary columns\n",
    "encoded_df = encoded_df.drop(columns=['Country', 'RegionName', 'Code', 'WineName', 'WineID', 'Vintages', 'Website', 'WineryID', 'WineryName', 'RegionID','Grapes','Harmonize'])\n",
    "encoded_df = encoded_df.dropna()\n",
    "# add new_harmonize to the encoded_df\n",
    "encoded_df = pd.concat([encoded_df, new_harmonize], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the IQR\n",
    "numeric_df = encoded_df['ABV']\n",
    "q1 = numeric_df.quantile(0.25)\n",
    "q3 = numeric_df.quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "# Boxplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=numeric_df, showmeans=True, orient=\"h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the resulting boxplot, we observe that some values fall outside the ranges but are still relevant, as 0% alcohol wines, including dealcoholized varieties, remain within the wine category. These options retain the essence of traditional wines without alcohol. Additionally, beverages like firewater, with higher alcohol content, can also bring value to the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix without the Elaborate columns to reduce the size of the heatmap\n",
    "elaborate_columns = encoded_df.columns[encoded_df.columns.str.startswith('Elaborate_')]\n",
    "matrix_df = encoded_df.drop(columns=elaborate_columns)\n",
    "correlation_matrix = matrix_df.corr()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(\n",
    "    correlation_matrix, \n",
    "    annot=True, \n",
    "    fmt=\".2f\", \n",
    "    cmap=\"coolwarm\", \n",
    "    cbar=True, \n",
    "    annot_kws={\"size\": 10}\n",
    ")\n",
    "plt.xticks(fontsize=12, rotation=45)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the matrix, we observe that it is relatively ‘cold,’ meaning it exhibits a low correlation index overall. Despite the generally low correlations between features, certain specific relationships do stand out. For instance, some dishes in the ‘Harmonize’ category show a notable correlation with particular wines, while certain body characteristics correlate with the alcohol by volume (ABV) and wine types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "harmonize_columns = [col for col in encoded_df.columns if col.startswith('Harmonize_')]\n",
    "\n",
    "# Loop through each 'harmonize_' column and perform PCA\n",
    "for label_col in harmonize_columns:\n",
    "    print(f\"Generating PCA plot for label column: {label_col}\")\n",
    "\n",
    "    features = encoded_df.drop(columns=[label_col])\n",
    "    labels = encoded_df[label_col].values\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    df_pca = pca.fit_transform(features)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    unique_labels = np.unique(labels)\n",
    "    for label in unique_labels:\n",
    "        plt.scatter(\n",
    "            df_pca[labels == label, 0], \n",
    "            df_pca[labels == label, 1], \n",
    "            label=f'Classe {label}'\n",
    "        )\n",
    "\n",
    "    plt.title(f'PCA - Column: {label_col}')\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applying PCA to identify patterns and relationships among the different categories in the ‘Harmonize’ column, we encountered inconclusive results. The resulting plots showed the various categories exhibiting similar patterns, with data points widely scattered across the graphical space rather than concentrated in specific zones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Type Prediction using Random Forest Classifier: Model Performance & Results - Task [089]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 68\u001b[0m\n\u001b[0;32m     65\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(pipeline, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Train the model using GridSearchCV without SMOTE\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Get the best model and parameters\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:1023\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1017\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1018\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1019\u001b[0m     )\n\u001b[0;32m   1021\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1023\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:1570\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1569\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1570\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:969\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    963\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    965\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    966\u001b[0m         )\n\u001b[0;32m    967\u001b[0m     )\n\u001b[1;32m--> 969\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    989\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Step 1: Define feature and target columns\n",
    "feature_columns = [col for col in encoded_df.columns if col.startswith('Harmonize_')]\n",
    "target_columns = [col for col in encoded_df.columns if col.startswith('Type_')]\n",
    "\n",
    "# Step 2: Split data into features (X) and target (y)\n",
    "X = encoded_df[feature_columns]\n",
    "y = encoded_df[target_columns]\n",
    "\n",
    "# Step 3: Split data into training, validation, and test sets (80% train, 20% test)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Encode target labels (multiclass classification)\n",
    "y_train_labels = y_train.idxmax(axis=1).str.replace(\"Type_\", \"\", regex=True)\n",
    "y_val_labels = y_val.idxmax(axis=1).str.replace(\"Type_\", \"\", regex=True)\n",
    "y_test_labels = y_test.idxmax(axis=1).str.replace(\"Type_\", \"\", regex=True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_labels = label_encoder.fit_transform(y_train_labels)\n",
    "y_val_labels = label_encoder.transform(y_val_labels)\n",
    "y_test_labels = label_encoder.transform(y_test_labels)\n",
    "\n",
    "# Step 5: Define Random Forest model and hyperparameter grid\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [5, 50, 100, 200],\n",
    "    'classifier__max_depth': [3, 5, 7, None],\n",
    "    'classifier__min_samples_split': [2, 5],\n",
    "    'classifier__min_samples_leaf': [1, 2],\n",
    "    'classifier__max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Step 6: Create a Random Forest pipeline (model with preprocessing)\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Step 7: Hyperparameter tuning using GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Step 8: Train the model using GridSearchCV\n",
    "grid_search.fit(X_train, y_train_labels)\n",
    "\n",
    "# Step 9: Retrieve the best model and parameters\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Step 10: Make predictions on the test set\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Step 11: Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test_labels, y_pred)\n",
    "classification_rep = classification_report(y_test_labels, y_pred, target_names=label_encoder.classes_)\n",
    "cm = confusion_matrix(y_test_labels, y_pred)\n",
    "\n",
    "# Step 12: Display results\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Classification Report:\\n{classification_rep}\")\n",
    "\n",
    "# Step 13: Plot Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Conclusions\n",
    "\n",
    "### 1. **Target: Type_Dessert**\n",
    "- **Accuracy**: 82.41%\n",
    "- **Key Findings**:\n",
    "  - The model performs excellently for the majority class (\"False\") with very high precision (1.00), but struggles with the minority class (\"True\"), showing very low precision (0.10) and a low F1-score (0.19).\n",
    "  - Class imbalance is still evident, with the majority class dominating predictions.\n",
    "  - **Recommendation**: Focus on improving the model's ability to predict the minority class (True) without compromising performance on the majority class.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Target: Type_Dessert/Port**\n",
    "- **Accuracy**: 97.65%\n",
    "- **Key Findings**:\n",
    "  - The model achieves high accuracy and precision for the majority class (\"False\") but has low precision (0.29) for the minority class (\"True\").\n",
    "  - Recall for the \"True\" class is very high (0.96), indicating that the model identifies the minority class well, but the low precision indicates a high number of false positives.\n",
    "  - **Recommendation**: Focus on improving precision for the \"True\" class while maintaining high recall.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Target: Type_Red**\n",
    "- **Accuracy**: 96.27%\n",
    "- **Key Findings**:\n",
    "  - The model performs well for both the \"False\" and \"True\" classes with balanced precision and recall.\n",
    "  - High F1-scores for both classes indicate that the model has learned to handle the class imbalance effectively.\n",
    "  - **Recommendation**: No immediate improvements are needed, but there could still be potential for further fine-tuning to optimize precision on the \"False\" class.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Target: Type_Rosé**\n",
    "- **Accuracy**: 91.77%\n",
    "- **Key Findings**:\n",
    "  - High precision for \"False\" but very low precision for \"True\".\n",
    "  - While recall for \"True\" is high (0.95), the low precision (0.34) and low F1-score (0.51) indicate room for improvement in identifying the \"True\" class without many false positives.\n",
    "  - **Recommendation**: Improve the balance between recall and precision for the \"True\" class.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Target: Type_Sparkling**\n",
    "- **Accuracy**: 90.63%\n",
    "- **Key Findings**:\n",
    "  - The model is better at predicting the \"False\" class, with high precision (0.99) and decent recall for the \"True\" class.\n",
    "  - However, the F1-score for the \"True\" class is still low (0.59) due to the class imbalance.\n",
    "  - **Recommendation**: Focus on improving the performance of the model for the \"True\" class while maintaining good recall for the \"False\" class.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Target: Type_White**\n",
    "- **Accuracy**: 94.29%\n",
    "- **Key Findings**:\n",
    "  - High precision for both classes with a notable improvement in predicting the minority class (\"True\").\n",
    "  - Recall for \"True\" is very high (0.98) and the F1-score (0.91) indicates balanced performance.\n",
    "  - **Recommendation**: Continue fine-tuning the model to maintain high precision and recall for both classes.\n",
    "\n",
    "---\n",
    "\n",
    "### **General Conclusion**:\n",
    "- **Class Imbalance**: All models still exhibit some degree of bias towards the majority class (\"False\") due to class imbalance.\n",
    "- **Next Steps**:\n",
    "  - **Hyperparameter Tuning**: Continue fine-tuning the hyperparameters, particularly `max_features`, `min_samples_split`, and `n_estimators`, to further improve performance.\n",
    "  - **Class Imbalance Solutions**: Since SMOTE was excluded from the preprocessing, techniques like adjusting class weights or exploring undersampling methods could help address the imbalance.\n",
    "  - **Threshold Optimization**: Explore optimizing the decision threshold for classification to balance precision and recall, especially for the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regression (SVR) for Wine Price Prediction: Model Performance & Results - Task [091]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../datasets/WineDataset.csv')\n",
    "\n",
    "# Clean the 'Price' column (remove currency symbol, commas, and text like 'per bottle')\n",
    "df['Price'] = df['Price'].replace({'Â£': '', ',': '', ' per bottle': ''}, regex=True)\n",
    "\n",
    "# Convert the 'Price' column to numeric, coercing errors to NaN (for any unexpected text)\n",
    "df['Price'] = pd.to_numeric(df['Price'], errors='coerce')\n",
    "\n",
    "# Check for and handle missing values in the 'Price' column\n",
    "missing_values = df['Price'].isnull().sum()\n",
    "print(f\"Missing values in 'Price' column: {missing_values}\")\n",
    "\n",
    "# Impute missing values for 'Price' with the median value\n",
    "df['Price'] = df['Price'].fillna(df['Price'].median())\n",
    "\n",
    "# Feature columns (Grape, Type)\n",
    "categorical_features = ['Grape', 'Type']\n",
    "y = df['Price']\n",
    "\n",
    "# Step 1: One-Hot Encode the categorical features ('Grape', 'Type')\n",
    "X_categorical = df[categorical_features]\n",
    "X_encoded = pd.get_dummies(X_categorical, drop_first=True)\n",
    "\n",
    "# Step 2: Standardize the data (important for SVR)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_encoded)\n",
    "\n",
    "# Step 3: Split data into train and test sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Hyperparameter Tuning with Loop for different values of 'C', 'epsilon', and 'kernel'\n",
    "results = []\n",
    "\n",
    "C_values = [1, 10, 100, 1000]\n",
    "epsilon_values = [0.01, 0.1, 0.2]\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "\n",
    "for C in C_values:\n",
    "    for epsilon in epsilon_values:\n",
    "        for kernel in kernels:\n",
    "            # Initialize the SVR model with current hyperparameters\n",
    "            svr_model = SVR(C=C, epsilon=epsilon, kernel=kernel)\n",
    "            \n",
    "            # Train the model\n",
    "            svr_model.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict on the test set\n",
    "            y_pred = svr_model.predict(X_test)\n",
    "            \n",
    "            # Evaluate the model's performance\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            # Store the results\n",
    "            results.append({\n",
    "                'C': C,\n",
    "                'epsilon': epsilon,\n",
    "                'kernel': kernel,\n",
    "                'MAE': mae,\n",
    "                'R2': r2\n",
    "            })\n",
    "\n",
    "# Convert results to a DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Step 5: Find the best parameters from the results\n",
    "best_params = results_df.loc[results_df['R2'].idxmax()]\n",
    "print(f\"Best parameters: C={best_params['C']}, epsilon={best_params['epsilon']}, kernel={best_params['kernel']}\")\n",
    "print(f\"Best R² Score: {best_params['R2']}\")\n",
    "\n",
    "# Step 6: Visualizing the results - Actual vs Predicted Price for the best model\n",
    "best_model = SVR(C=best_params['C'], epsilon=best_params['epsilon'], kernel=best_params['kernel'])\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_test, y=y_pred_best, color='blue', label='Predictions')\n",
    "sns.scatterplot(x=y_test, y=y_test, color='red', label='Actual')\n",
    "plt.title('SVR - Actual vs Predicted Price')\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 7: Plot the results from grid search (C vs epsilon vs R2 Score)\n",
    "pivot_table = results_df.pivot_table(index='C', columns='epsilon', values='R2', aggfunc='max')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(pivot_table, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('R² Score Heatmap for Different C and Epsilon Values')\n",
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('C')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Based on the results of the **Support Vector Regression (SVR)** model with various hyperparameters, the following conclusions can be made:\n",
    "\n",
    "### Best Model Parameters:\n",
    "- The best performing model used the following hyperparameters:\n",
    "  - **C**: 1000\n",
    "  - **Epsilon**: 0.01\n",
    "  - **Kernel**: Radial Basis Function (RBF)\n",
    "\n",
    "### Model Performance:\n",
    "- The **R² score** for the best model is **0.0582**. This indicates that the model explains only **5.8%** of the variance in the target variable (Price), suggesting that the model has a relatively poor fit to the data.\n",
    "- The **Mean Absolute Error (MAE)** is approximately **13.34**, meaning the model’s predictions deviate by about **13.34 units** on average, which could be significant depending on the scale of wine prices.\n",
    "\n",
    "### Model Evaluation:\n",
    "- Despite tuning the hyperparameters, the performance of the model remains modest, as indicated by the low R² score and moderate MAE.\n",
    "- The R² score close to zero implies that there is little to no correlation between the features (Grape and Type) and the price of the wine. This suggests that other factors, not included in the model, could have a significant impact on wine pricing.\n",
    "\n",
    "### Possible Improvements:\n",
    "- Including additional features (e.g., alcohol content, region, year of production) could enhance the model's predictive power.\n",
    "- More advanced techniques such as **feature engineering** might reveal hidden relationships in the data.\n",
    "- Exploring other machine learning models (e.g., Random Forest, Gradient Boosting) or ensemble methods could lead to better results.\n",
    "\n",
    "### Summary:\n",
    "In conclusion, the **SVR model** with the best hyperparameters offers some predictive capability but does not adequately explain the variance in wine prices. To improve its accuracy, further refinement of the model, the inclusion of additional features, or the use of different algorithms is recommended.\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation for the Context:\n",
    "\n",
    "**SVR (Support Vector Regression)** is a type of machine learning model used to predict continuous values. \n",
    "\n",
    "- **Target**: In this case, the target variable is the **price of wine**.\n",
    "- **Variables**: The model uses the **grape variety** and **type of wine** as input features to predict the price of the wine.\n",
    "\n",
    "In summary, this model attempts to predict the price of wine based on its **grape variety** and **type**, but the results indicate that these features alone are not sufficient to explain the variation in wine prices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
